{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/drscotthawley/DLAIE/blob/main/Lessons/Week1.ipynb)\n",
    "\n",
    "## Week 1: Not All ML is DL\n",
    "\n",
    "![equation](https://latex.codecogs.com/png.download?DL%5Csubset%20ML%5Csubset%20AI)\n",
    "\n",
    "MathJax: \n",
    "\n",
    "$$ DL \\subset ML \\subset AI $$\n",
    "\n",
    "Someday this course will occur after other machine learning (ML) courses in a multi-course sequence. It was by the request of the Data Science program that we focus on DL here, which I was happy to do, because DL is what I know best. \n",
    "\n",
    "Still, we're going spend some time reviewing some key topics that appear in non-DL ML since they form the basis of what will come next.\n",
    "\n",
    "### Structured Data and Features\n",
    "\n",
    "The kinds of data that many businesses deal with are already highly structured, such as having a set of columns in a spreadsheet which each describe aspects of a data item (or \"data point\").  For housing data, a given row in a spreadsheet might tell you various \"features\" of a house such as its zip code, last selling price, and...IDK, more housing-related stuff.   Or perhaps you have a dataset on people, and the columns in each row make up a data point for one person, describing \"features\" about them their height, weight, age, gender, whether or not they are a smoker, and so on. \n",
    "\n",
    "These features are often highly descriptive and immediately useful for traditional machine learning systems such as Random Forests models -- business-based ML engineers get a lot of mileage out of Random Forests, by the way (because, again, they're often working with highly-structured, spreadsheet-like data).  We're not going to cover those models in this course, but we mention them here to note that\n",
    "\n",
    ">  If you've already got highly meaningful features for your data, you don't \"need\" neural networks (NNs).\n",
    "\n",
    "You *could* use a NN (and we'll do examples using NNs) but you could just as easily -- more easily, even! -- start with something simple like a linear model  (statisticians get a lot of mileage out of linear models BTW) or a Random Forest ,and that would be a traditional approach which works very well and makes people lots of money. \n",
    "\n",
    "But... \n",
    "\n",
    "What if you don't have well-defined features?  What if you want to extract information from images, or audio, or raw text?\n",
    "\n",
    "**Then you still don't \"need\" a NN!** Plenty of feature-extraction techniques have been developed over the decades to find important aspects of images, audio, text, and more, and then feed these into traditional ML or stats-based models!  An example you might be familiar with would be face-detection algorithms for camera-based apps: these don't need NNs to find out where your eyes & nose & mouth are!  And they can execute very quickly.  \n",
    "\n",
    "Here's a video of me being silly, using a (non-NN) \"face detector\" algorithm which acts an input to a ML algorithm that controls the pitch of an audio oscillator:\n",
    "\n",
    "We're inside a Markdown cell, should I should be able to include arbitrary HTML, so here's the YouTube iframe embed code:\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/j8S9Cx0xnbs\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "...but in the notebook all I see is a big blank space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shawley/opt/anaconda3/lib/python3.7/site-packages/IPython/core/display.py:701: UserWarning: Consider using IPython.display.IFrame instead\n",
      "  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/j8S9Cx0xnbs\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instead \"they\" recommend using a code cell to import an iframe, so I'll do that. This works fine in Jupyter\n",
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/j8S9Cx0xnbs\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')\n",
    "#...but doesn't seem to render anything when viewed on GitHub. \n",
    "\n",
    "# And anyway, I don't want to display a block of code just to get a video to show. My \n",
    "# searching into code-hiding of cells...seems like it's really complicated! \n",
    "# and what's with this warning message about \"consider IFrame instead\"? No. I'll use HTML thank you very much!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's try\n",
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"j8S9Cx0xnbs\")\n",
    "# this also renders in Jupyter, let's see if it works in GitHub!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...sadly it does not render in GitHub.  But it will render in Colab! \n",
    "\n",
    "\n",
    "**TODO: SAY MORE**\n",
    "\n",
    "### Not all NNs are DNNs\n",
    "\n",
    "The simple neural networks (NNs) we will start with will not be \"deep\" neural networks (DNNs); \"deep\" refers to having many \"layers\" of artificial neurons.  Advances in algorithms over the past two decades have made it feasible to train very deep networks -- sometimes having thousands of layers -- which previously were an impossibility. As a [heuristic], we can think of \"earlier\" layers and being feature extractors for \"later\" layers, where the former are closer to the inputs and the latter are closer to the outputs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
