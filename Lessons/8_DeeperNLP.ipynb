{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "8_DeeperNLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmeJzNsGP41j"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1blkWFKMYqinrtTSKWO_Uvv7v_TvtX6zs?usp=sharing)\n",
        "\n",
        "# Lesson 8: Going Deeper into NLP\n",
        "\n",
        "[Previously](https://github.com/drscotthawley/DLAIE/blob/main/Lessons/7_NLP_via_HuggingFace_Transformers.ipynb) we saw how convenient it was to use the `pipeline` method of the HuggingFace.co `transformers` library to perform a variety of Natural Language Processing (NLP) tasks. But there's a lot going on under the hood that was hidden from us.  If we want to learn how these models work, we're going to have to peel back several layers, on multiple levels.  \n",
        "\n",
        "What we did in the previous NLP lesson was a bit like watching a big rocket take off from a distance. There are many systems in the rocket that are all working together to effect the launch.  To understand how the big rocket operates, it will help if we go back to study smaller, simpler rockets so that we understand the principles of rocketry. \n",
        "\n",
        "In this lesson we'll learn the parts of an NLP model and see how they go together. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGWoRTrURumS"
      },
      "source": [
        "## 1. Tokenization\n",
        "\n",
        "Whatever NLP task we're interested in performing, there will be a large amount of text (sometimes called a \"corpus\") that we will use for training the model on. That text needs to be split up somehow into bite-sized parts to operate upon. This process is known as *tokenization*. We could try treating individual characters as tokens, or [regard entire sentences as our tokens](https://claritynlp.readthedocs.io/en/latest/developer_guide/algorithms/sentence_tokenization.html), but a common mid-point is to use *words* as tokens.  \n",
        "\n",
        "> *For a great example of a character-based neural network, see [Andrej Karpaty's Char-RNN](https://github.com/karpathy/char-rnn). \\[OPTIONAL, not required\\]\n",
        "\n",
        "The simplest -- and typically *the default* -- scheme for word-level tokenization is just to split the text at every space and at every punctuation mark. Let's try an example\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQyxw7I4zpJf"
      },
      "source": [
        "So for instance, the follwing sample text:\n",
        "```\n",
        "I'm going to the store, because I need some milk.\n",
        "```\n",
        "might become\n",
        "```\n",
        "[\"I\", \"'\", \"m\", \"going\", \"to\", \"the\",  \"store\", \",\", \"because\", \"I\", \"need\", \"some\", \"milk\", \".\"]\n",
        "```\n",
        "Tokenization is something that many computational linguists have spent a great deal of time on, and there are [a variety of tokenizers](https://towardsdatascience.com/overview-of-nlp-tokenization-algorithms-c41a7d5ec4f9?gi=73a2ec14356e) available. Generally it's generally in our best interest to just call a library such as[Natural Language Toolkit (NLTK)](https://www.nltk.org/) to do the tokenizing for us instead of trying to do it from scratch. Both FastAI and HuggingFace allow us to choose between a variety of tokenizers.  (FastAI's default tokenizer is currently from the [spaCy NLP library](https://spacy.io/).)\n",
        "\n",
        "Let's try an actual example using the NLTK word tokenizer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVfwpXS703Os",
        "outputId": "dc1c5932-1b54-430a-9c6d-fe7217c3fba5"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')    # this is a resource needed by NLTK\n",
        "sentence = \"I'm going to the store, because I need some milk.\"\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "print(\"tokens = \",tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "tokens =  ['I', \"'m\", 'going', 'to', 'the', 'store', ',', 'because', 'I', 'need', 'some', 'milk', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSr3MFM208sK"
      },
      "source": [
        "Interesting that the apostrophe from \"I'm\" went with the \"m\" (as in \"'m\") instead of being its own thing. Presumably this is so we can then expand it into \"am\".  What about the \"n\" in \"don't\"? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWotEHBA1Onf",
        "outputId": "8af10c7b-20d4-447c-8aeb-40cac0f2fe11"
      },
      "source": [
        "sentence2 = \"I don't know what's going to happen in this case, but it should be interesting!\"\n",
        "tokens = nltk.word_tokenize(sentence2)\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'do', \"n't\", 'know', 'what', \"'s\", 'going', 'to', 'happen', 'in', 'this', 'case', ',', 'but', 'it', 'should', 'be', 'interesting', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVcdMXA01U7F"
      },
      "source": [
        "In this case the \"n\" from \"don't\" went with the \"'t\". Again, this best facilitates filling in the missing \"o\".  Let's try some spirited Tennessee-style language:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGrenE9u1hcd",
        "outputId": "03ca0c1b-0bf5-45b5-82ce-e2eb06a1eb1f"
      },
      "source": [
        "sentence3 = \"I'm fixin' to spend $1499.95 on a new four wheeler and you ain't gonna stop me, ma!\"\n",
        "print(nltk.word_tokenize(sentence3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', \"'m\", 'fixin', \"'\", 'to', 'spend', '$', '1499.95', 'on', 'a', 'new', 'four', 'wheeler', 'and', 'you', 'ai', \"n't\", 'gon', 'na', 'stop', 'me', ',', 'ma', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raD2F2CE2K92"
      },
      "source": [
        "Wow, it knows \"you'uns\"!  And it splits \"gonna\", presumably in preparation for a mapping to \"going\", \"to\".\n",
        "\n",
        "Do we need the commas and exclamation points though?  Maybe, maybe not.  It depends on our use case.  Sometimes other punctuation is relevant, such as hashtags and @-symbols for social media.  NLTK has a special tokenizer for Twitter:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ib1dPC01j24G",
        "outputId": "88691815-a67b-497c-ceb7-80cbeda5fd25"
      },
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "tt = TweetTokenizer()\n",
        "\n",
        "tweet = \"OMG I love @SuperFamousPerson's new look! #fridays #nofilter\"\n",
        "\n",
        "# Let's compare the two tokenizers:\n",
        "print(\"Regular word tokenizer:\", nltk.word_tokenize(tweet))\n",
        "print(\"Tweet tokenizer:       \",tt.tokenize(tweet))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regular word tokenizer: ['OMG', 'I', 'love', '@', 'SuperFamousPerson', \"'s\", 'new', 'look', '!', '#', 'fridays', '#', 'nofilter']\n",
            "Tweet tokenizer:        ['OMG', 'I', 'love', '@SuperFamousPerson', \"'\", 's', 'new', 'look', '!', '#fridays', '#nofilter']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVd1JdCSk5mg"
      },
      "source": [
        "...So the specialty `TweetTokenizer` kept certain kinds of punctuation with their associated words, rather then splitting at all forms of punctuation like the regular word tokenizer did."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqXeh85J02ZM"
      },
      "source": [
        "\n",
        "\n",
        "Beyond the question of which punctuation to keep, we must also recognize that words come in a variety of forms.  And some words may be \"filler\" that we may not need for the task at hand (e.g., articles like \"a\", \"an\", and \"the\" are often discarded).  So we may wish to regard related words such as \"jump\", \"jumping\", \"jumps\",... as variations on the *stem* of \"jump\".  We may hang on to the endings such as \"-ing\" for later use, regarding them as additional tokens. The process of *stemming* or \"*stemmification*\" is the breaking up of words into their stems and hanging on to endings (or not).  Also, what about compound words?  Some languages such as German will make very long single words (e.g. Geschwindigkeitsbegrenzung for \"speed limit\") that in other languages would be considered as separate words. If language translation is our goal, some way of tokenizing that includes such variability would be important.  Also, what about punctuation? To keep things simple, we could just delete all forms of punctuation -- or expand contractions like \"I'll\" to \"I will\", and so forth -- and yet if we want a highly accurate model we may find that holding on to some forms of punctuation will important.\n",
        "\n",
        "\n",
        "\n",
        "####  Special Token Codes\n",
        "Often language models will make use of special tokens such as `UNK` (a token to substitute for unknown words) or `PAD` (for extra padding words), or `EOS` (end of sentence), depending on the task at hand. Sometimes these will have extra characters like `<UNK>` or `[UNK]`. There may or may not be `<START>` and `<END>` tokens for the beginning and end of the text.  The exact list of special tokens depends on the tokenizer and the model, but those few are pretty universal. So when you see those, in what follows, you'll be prepared.  \n",
        " \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5K6M4aTmWLW"
      },
      "source": [
        "## Numericalization & Word Vectors\n",
        "Once we have the tokens, we still need to convert these into numbers somehow so we can operate on them mathematically. Depending on the application, different numericalization schemes are available. \n",
        "\n",
        "One *very simple* way to do this if we were, say, doing *Sentiment Classification* in tweets, movie reviews, or other kinds of \"posts\",  would be to count the frequency of all the words that appear in positive posts, and do the same for all the negative posts.  Expressing these frequencies as fractions of the total number of words, we could then assign to each word its pair of \"positive use\" and \"negative use\" frequency values $(f_p, f_n)$ which lie in the two-dimensional [unit square](https://en.wikipedia.org/wiki/Unit_square) (shown below). These would then form the coordinates for a *word vector* of our word in its *embedding* space (i.e., the unit square in this case).  Then to classify a post, we could just take the sum of the word vectors of all the words in the post and see whether the result is more \"positive\" than \"negative\". In other words, we could ask, which region of the following embedding diagram does the mean of the word vectors in the post lie in?\n",
        "\n",
        "![img of regions of positive and negative](https://i.imgur.com/mLpQHBj.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqpcM4r6tpO-"
      },
      "source": [
        "That might suffice as a simple baseline model, and it might work \"ok\", but there are issues with it. For example, it's possible that different words could get mapped to the exact same point.  If all you care about is how positive or how negative the post (or tweet, or review) is, this may not be a problem,  but if you want to \"understand\" the text, produce a translation of it, or generate new text, then this method is useless.  Another issue is that words that mean almost the same thing but are used with different frequencies (e.g. \"amazing\" and \"stupendous\") would receive very different word vectors, even though we'd want them to have essentially the same effects on the model's output.\n",
        "\n",
        "> Terminology: our simplistic method of just summing up the word vectors together pays no attention to the *order* of the words, so the above model would be termed a \"Bag of Words\" type of model.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfFS04zavBVD"
      },
      "source": [
        "In order to help preserve uniqueness as well as to better allow words to express their ranges of meanings, one typically uses many more than two dimensions for word vector embeddings.  It's quite common to see 256 or more (e.g. 300) dimensions for words.  While these are too many dimensions to visualize (which is why I gave the simple example above!) the computer is able to deal with them just fine.  \n",
        "\n",
        "The way one typically gets these word vectors is to take in the list of all the (unique) words in the corpus and produce a \"vocabulary\" which indexes the words and generates a one-hot encoding by treating the words as categories.  Then we map these categories into word vectors via a matrix of trainable weights. So, for example, a corpus with 10,000 unique words mapped into 300-dimensional word vectors would involve a weights matrix of 300\\*10000 = 3 million weights. \n",
        "\n",
        "\\[TODO: Add a picture someday! ;-) \\] \n",
        "\n",
        "Thus *the \"embedding\" mapping is itself a neural network* which we train as the front-end of our full (larger) neural network.\n",
        "This means that the more words you allow in your vocabulary (or \"vocab\"), the bigger that initial embedding operation will be.  Typically, in order to keep this matrix from getting too big, one will truncate the list of words by removing the less frequent or less important words from the vocab and replacing them with special tokens such as `UNK`. \n",
        "The form the embedding takes may depend on the task.  \n",
        "\n",
        "## Language Modeling as a Pretraining Task\n",
        "One very useful method is to use a *language model* task to produce word embeddings.  A language model tries to predict the next word in a sequence given its preceding words (how many preceding words you use determines the sophistiation of the model). This forms a \"self-supervised learning\" method in the sense that the target data you train on is the same as the input data, just shifted ahead by one word. \n",
        "\n",
        "This approach was used to great effect by Jeremy Howard and Sebastian Ruder in their [ULMFit paper](https://paperswithcode.com/method/ulmfit), in which they used a language model task of predicting the next word in Wikipedia (specifically, the [Wikitext-103](https://paperswithcode.com/dataset/wikitext-103) dataset) in order to condition the model to use for other tasks such as sentiment analysis of IMDB movie reviews.  Their result was that they beat other competing sentiment analysis methods by a longshot!  \n",
        "\n",
        "The idea is that a model that has to predict the next word in a large text has to develop somewhat of an \"understanding\" of how language works, and thus will be a more powerful model for text classification than a simpler model that \n",
        "\n",
        "> Note: A neat effect of this form of pre-training is that you also end up with a text generation model.\n",
        "\n",
        "Now, we're not going to train a model on Wikipedia right now.  That would be a waste of time, as we can just download pretrained weights and go from there.  Let's use the fastai set of methods for doing this, and we'll work through their IMDB example problem [as described in Chapter 10 of the `fastbook`](https://github.com/fastai/fastbook/blob/master/10_nlp.ipynb).  To get started we'll need to download the dataset and start using fastai's tokenizer(s)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEjn2JVGP0Ms"
      },
      "source": [
        "!pip install -Uqq fastai fastbook"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLEk7zgv41Uf"
      },
      "source": [
        "# if the next line produces an error, restart the runtime and try again.\n",
        "import fastbook  \n",
        "from fastai.text.all import *\n",
        "from IPython.display import display, HTML"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIUqbBRN4nRB"
      },
      "source": [
        " path = untar_data(URLs.IMDB)  # download the dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lp0X73dm4X3R",
        "outputId": "b40e1ab7-ef2c-4414-9c64-1992c9d526f8"
      },
      "source": [
        "# make a list of all the files in all the folders of the dataset\n",
        "files = get_text_files(path, folders = ['train', 'test', 'unsup'])\n",
        "\n",
        "# let's look at the first 75 characters of the first file in the list\n",
        "txt = files[0].open().read();  txt[:75]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Kannathil Muthamittal is for sure a great movie. I have to give it to Mani '"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcU9CTZD6hG8"
      },
      "source": [
        "As we mentioned above the current default tokenizer in FastAI is from the spaCy NLP package:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGshRMNu6bzS",
        "outputId": "96db2534-70a1-413b-988c-39c5bbfe9e21"
      },
      "source": [
        "spacy = WordTokenizer()\n",
        "spacified = spacy([txt])  \n",
        "print(spacified)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<generator object SpacyTokenizer.__call__.<locals>.<genexpr> at 0x7f9f15d77f50>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeCAdqIn7K6u"
      },
      "source": [
        "So the word tokenizer is a generator. In order to access its output we can use `first()` and `next()`:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrcspiSf6pUx",
        "outputId": "782ecd8d-94e6-4e4d-a47b-7be714bb2167"
      },
      "source": [
        "toks = first(spacy([txt]))\n",
        "print(toks)   # This prints out all the tokens\n",
        "print(coll_repr(toks, 30))  # fastai's coll_repr method gives the total size and first N (=30) tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Kannathil', 'Muthamittal', 'is', 'for', 'sure', 'a', 'great', 'movie', '.', 'I', 'have', 'to', 'give', 'it', 'to', 'Mani', 'Ratnam', 'for', 'a', 'great', 'directing', 'job', 'and', 'A.R.', 'Rahman', 'for', 'great', 'songs', '.', 'The', 'camera', 'work', 'is', 'just', 'excellent', 'and', 'is', 'similar', 'to', 'Black', 'Hawk', 'Down', 'and', 'Saving', 'Private', 'Ryan', '.', 'I', 'will', 'be', 'shocked', 'if', 'this', 'movie', 'does', 'not', 'win', 'an', 'Oscar', 'for', 'Best', 'Foreign', 'Film', 'or', 'even', 'Best', 'Camera', 'Work', '.']\n",
            "(#69) ['Kannathil','Muthamittal','is','for','sure','a','great','movie','.','I','have','to','give','it','to','Mani','Ratnam','for','a','great','directing','job','and','A.R.','Rahman','for','great','songs','.','The'...]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOcUtcWh8Gtp"
      },
      "source": [
        "In addition to `WordTokenizer`, fastai adds some extra functionality via a `Tokenizer` method, that will turn all words to lower case but precede such interventions with a special code `xxmaj` indicating that the next word should be capitalized.  It also adds `xxbos` to denote the beginning of the sentence. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_k9AH3Sj761H",
        "outputId": "134f7ab1-82e0-4582-9b27-f2309457f473"
      },
      "source": [
        "tkn = Tokenizer(spacy)\n",
        "print(coll_repr(tkn(txt), 31))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(#91) ['xxbos','xxmaj','kannathil','xxmaj','muthamittal','is','for','sure','a','great','movie','.','i','have','to','give','it','to','xxmaj','mani','xxmaj','ratnam','for','a','great','directing','job','and','xxup','a.r','.'...]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5wJRkJ18trP"
      },
      "source": [
        "> Note: fastai also has a tokenization method that will use sub-words -- i.e., groups of characters -- but we're going to skip that part for now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfZq-2xc89AS"
      },
      "source": [
        "To do calculations on the GPU, it's helpful to work with \"batches\" of data, just like we did for images.  In each batch we need the same demensions, so we will chop the text up into \"chunks\" of length `seq_len` and then group these into batches.  Rather than totally randomly assigning the order of the batches, we will have the model \"read\" the text sequentially, where each new element of a batch will simply be shifted ahead one word. \n",
        "\n",
        "See this fastai example where they use a batch size of `bs=6` and sequence length of `seq_len=5` to produce one batch from a sample text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbhnhqg3-X0M",
        "outputId": "e8f5766d-d011-46b2-c9d3-5591ab175e6f"
      },
      "source": [
        "stream = \"In this chapter, we will go back over the example of classifying movie reviews we studied in chapter 1 and dig deeper under the surface. First we will look at the processing steps necessary to convert text into numbers and how to customize it. By doing this, we'll have another example of the PreProcessor used in the data block API.\\nThen we will study how we build a language model and train it for a while.\"\n",
        "print(stream)\n",
        "tokens = tkn(stream)\n",
        "print(\"\\n\",len(tokens),\"tokens in stream.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In this chapter, we will go back over the example of classifying movie reviews we studied in chapter 1 and dig deeper under the surface. First we will look at the processing steps necessary to convert text into numbers and how to customize it. By doing this, we'll have another example of the PreProcessor used in the data block API.\n",
            "Then we will study how we build a language model and train it for a while.\n",
            "\n",
            " 90 tokens in stream.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VB_E9QMY_Y29"
      },
      "source": [
        "Although we could randomly grab \"chunks\" from all over the file and try to predict the word following each chunk, the fastai folks recommend making the text in each row of each batch follow immediately from the text in the corresponding row the previous batch.  Which means making some fancy slicing code like the following, in which we show three sequential batches.   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "id": "yMiGHdF67-ab",
        "outputId": "02db36f8-ad8f-4982-def4-70a9cb9ba99e"
      },
      "source": [
        "\n",
        "bs,seq_len = 6, 5                          # batch size and sequence length\n",
        "num_batches = len(tokens)// bs // seq_len  # 30 tokens per batch, 90 tokens = 3 batches. \n",
        "print(\"num_batches = \",num_batches)  \n",
        "num_rows = len(tokens) // seq_len          # total rows of all batches == 18\n",
        "print(\"num_rows = \",num_rows)\n",
        "\n",
        "for b in range(num_batches):\n",
        "    stride = seq_len * num_batches \n",
        "    d_tokens = np.array([tokens[i*stride + b*seq_len :i*stride + b*seq_len + seq_len] for i in range(bs)]) # i is the row number\n",
        "    df = pd.DataFrame(d_tokens)\n",
        "    print(f\"\\nbatch = {b}:\")\n",
        "    display(HTML(df.to_html(index=False,header=None)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_batches =  3\n",
            "num_rows =  18\n",
            "\n",
            "batch = 0:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>xxbos</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>in</td>\n",
              "      <td>this</td>\n",
              "      <td>chapter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>movie</td>\n",
              "      <td>reviews</td>\n",
              "      <td>we</td>\n",
              "      <td>studied</td>\n",
              "      <td>in</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>first</td>\n",
              "      <td>we</td>\n",
              "      <td>will</td>\n",
              "      <td>look</td>\n",
              "      <td>at</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>how</td>\n",
              "      <td>to</td>\n",
              "      <td>customize</td>\n",
              "      <td>it</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>of</td>\n",
              "      <td>the</td>\n",
              "      <td>preprocessor</td>\n",
              "      <td>used</td>\n",
              "      <td>in</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>will</td>\n",
              "      <td>study</td>\n",
              "      <td>how</td>\n",
              "      <td>we</td>\n",
              "      <td>build</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "batch = 1:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>,</td>\n",
              "      <td>we</td>\n",
              "      <td>will</td>\n",
              "      <td>go</td>\n",
              "      <td>back</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>chapter</td>\n",
              "      <td>1</td>\n",
              "      <td>and</td>\n",
              "      <td>dig</td>\n",
              "      <td>deeper</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>the</td>\n",
              "      <td>processing</td>\n",
              "      <td>steps</td>\n",
              "      <td>necessary</td>\n",
              "      <td>to</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>xxmaj</td>\n",
              "      <td>by</td>\n",
              "      <td>doing</td>\n",
              "      <td>this</td>\n",
              "      <td>,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>the</td>\n",
              "      <td>data</td>\n",
              "      <td>block</td>\n",
              "      <td>xxup</td>\n",
              "      <td>api</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>a</td>\n",
              "      <td>language</td>\n",
              "      <td>model</td>\n",
              "      <td>and</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "batch = 2:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>over</td>\n",
              "      <td>the</td>\n",
              "      <td>example</td>\n",
              "      <td>of</td>\n",
              "      <td>classifying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>under</td>\n",
              "      <td>the</td>\n",
              "      <td>surface</td>\n",
              "      <td>.</td>\n",
              "      <td>xxmaj</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>convert</td>\n",
              "      <td>text</td>\n",
              "      <td>into</td>\n",
              "      <td>numbers</td>\n",
              "      <td>and</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>we</td>\n",
              "      <td>'ll</td>\n",
              "      <td>have</td>\n",
              "      <td>another</td>\n",
              "      <td>example</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>.</td>\n",
              "      <td>\\n</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>then</td>\n",
              "      <td>we</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>it</td>\n",
              "      <td>for</td>\n",
              "      <td>a</td>\n",
              "      <td>while</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aE80zhFGFeXr"
      },
      "source": [
        "See how each row of each batch continues the text from the same row in the preceding batch?  Don't worry, you won't have to reproduce that code, fastai will do it internally.  \n",
        "\n",
        "When we were training images, we shuffled the order of images between epochs.  In the case of NLP we don't want to shuffle the words or even the rows.  Instead when we take a bunch of movie reviews and concatenate them to form a stream (which then broken into tokens and then batches), what we do is randomize the *order in which the reviews are concatenated* at each epoch.  This allows for word orderings to stay the same but where they appear in the training dataset to still shift around a bit in order to prevent overfitting. \n",
        "\n",
        "\n",
        "This is generally handled automatically by fastai, that will define the Tokenizer, set it up, and specify a Numericalize function, and set that up.  Here we show a brief example of that:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "D4SGCUpz9JrL",
        "outputId": "81e490a7-f02d-4bf6-ac73-b2b3862b5077"
      },
      "source": [
        "txts = L(o.open().read() for o in files[:2000])  # read texts of the first 2000 files\n",
        "txts[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Kannathil Muthamittal is for sure a great movie. I have to give it to Mani Ratnam for a great directing job and A.R. Rahman for great songs. The camera work is just excellent and is similar to Black Hawk Down and Saving Private Ryan. I will be shocked if this movie does not win an Oscar for Best Foreign Film or even Best Camera Work.'"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d1ngrlbGvEv",
        "outputId": "f76d6c75-dff5-4484-b879-22d5c2097b1c"
      },
      "source": [
        "toks200 = txts[:200].map(tkn)   # tokenize the first 200 files, by mapping the \"tkn\" function to the elements of text.\n",
        "toks200[0]  # show us the tokens corresponding to the text in the first file "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#91) ['xxbos','xxmaj','kannathil','xxmaj','muthamittal','is','for','sure','a','great'...]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "YBF0Z62qHJ3V",
        "outputId": "7886fee0-2ee5-4548-f185-5961259a98e1"
      },
      "source": [
        "num = Numericalize()\n",
        "num.setup(toks200)   # create a vocab for the stream we've created. \n",
        "coll_repr(num.vocab,20)  # show the first 20 words in the vocab, in order of descending frequency"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"(#1904) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj','the',',','.','and','a','of','to','is','in','it','i'...]\""
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DA4kJ85tHpJL"
      },
      "source": [
        "Then we can show how these individual tokens are rendered as numbers.  Note that the special codes get mapped to zero:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1QT3GCsHWYV",
        "outputId": "8d3945f3-524d-4aa4-d3be-e872c136bf30"
      },
      "source": [
        "nums200 = toks200.map(num);\n",
        "print(toks200[0])\n",
        "print(nums200[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['xxbos', 'xxmaj', 'kannathil', 'xxmaj', 'muthamittal', 'is', 'for', 'sure', 'a', 'great', 'movie', '.', 'i', 'have', 'to', 'give', 'it', 'to', 'xxmaj', 'mani', 'xxmaj', 'ratnam', 'for', 'a', 'great', 'directing', 'job', 'and', 'xxup', 'a.r', '.', 'xxmaj', 'rahman', 'for', 'great', 'songs', '.', 'xxmaj', 'the', 'camera', 'work', 'is', 'just', 'excellent', 'and', 'is', 'similar', 'to', 'xxmaj', 'black', 'xxmaj', 'hawk', 'xxmaj', 'down', 'and', 'xxmaj', 'saving', 'xxmaj', 'private', 'xxmaj', 'ryan', '.', 'i', 'will', 'be', 'shocked', 'if', 'this', 'movie', 'does', 'not', 'win', 'an', 'xxmaj', 'oscar', 'for', 'xxmaj', 'best', 'xxmaj', 'foreign', 'xxmaj', 'film', 'or', 'even', 'xxmaj', 'best', 'xxmaj', 'camera', 'xxmaj', 'work', '.']\n",
            "TensorText([   2,    8,    0,    8,    0,   16,   30,  273,   13,   72,   29,   11,   19,   45,   15,  223,   18,   15,    8,    0,    8,    0,   30,   13,   72,  469,  297,   12,    7,    0,   11,    8,\n",
            "           0,   30,   72,  470,   11,    8,    9,  426,  164,   16,   70,  191,   12,   16,  471,   15,    8,  298,    8,    0,    8,  170,   12,    8,    0,    8,    0,    8, 1069,   11,   19,  101,\n",
            "          46,    0,   74,   20,   29,   88,   40,  650,   49,    8,  581,   30,    8,   84,    8,    0,    8,   26,   59,  118,    8,   84,    8,  426,    8,  164,   11])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teZw3UP9H_Bo"
      },
      "source": [
        "^Note how the unknown / low frequency words get mapped to 0, which is the code for `UNK` (or \"xxunk\" in fastai parlance).\n",
        "\n",
        "These can then go into a fastai DataLoader which has been setup for language modeling, [`LMDataLoader`](https://docs.fast.ai/text.data.html#LMDataLoader), which is designed to load a batch of text as an input and the *same text shifted ahead by one word* as the target data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oE8oEJW8Hnu2",
        "outputId": "031f1fbd-004c-40b1-c2fa-eb9ac7c5358e"
      },
      "source": [
        "dl = LMDataLoader(nums200)\n",
        "\n",
        "# test it\n",
        "x,y = first(dl)\n",
        "print(x.shape,y.shape)\n",
        "\n",
        "# we can print out x & y but lets convert them from numbers to text when we view them\n",
        "print(', '.join(num.vocab[o] for o in x[0][:20]))\n",
        "print(', '.join(num.vocab[o] for o in y[0][:20]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 72]) torch.Size([64, 72])\n",
            "xxbos, xxmaj, xxunk, xxmaj, xxunk, is, for, sure, a, great, movie, ., i, have, to, give, it, to, xxmaj, xxunk\n",
            "xxmaj, xxunk, xxmaj, xxunk, is, for, sure, a, great, movie, ., i, have, to, give, it, to, xxmaj, xxunk, xxmaj\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RM15JYsCJWTX"
      },
      "source": [
        "See how each word in y is just the corresponding \"next word in x\" at the same index?  As a simple exercise, can you do the same?  Write a \"shift left\" function that just shifts a set of list elements to the left.  Add a \"xxpad\" on the end:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lCCCSuJIldm"
      },
      "source": [
        "## EXERCISE. Fill in your code below as directed\n",
        "\n",
        "def shift_left(orig:list):   \n",
        "    ## Your code below. Define a variable called \"shifted\" that is the original \n",
        "    #  list, shifted to the left by one, and filled in with a \"xxpad\" at the end.\n",
        " \n",
        "    shifted =  \n",
        " \n",
        "    ## end of your code\n",
        "    return shifted "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvAPBosZKsKE"
      },
      "source": [
        "Test your code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIYzPWbGKcQV",
        "outputId": "1dda733d-fb74-49fd-ad09-83cac5ef31f8"
      },
      "source": [
        "shift_left([1,2,3,4,5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 3, 4, 5, 'xxpad']"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hg427WeUKiEV"
      },
      "source": [
        "```\n",
        "Expected ouput:\n",
        "[2, 3, 4, 5, 'xxpad']\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omqK89l_KgJl"
      },
      "source": [
        "# and another check\n",
        "assert shift_left([]) == ['xxpad']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7slem8hLVod"
      },
      "source": [
        "---\n",
        "## Half-Way Point\n",
        "\n",
        "*(Ok, more like the 1/3 Point)*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfMqXDYjcqB_"
      },
      "source": [
        "## More Exercises!\n",
        "\n",
        "Huggingface and fastai will end up hiding a lot of what's happening from us, so let's try writing a few more simple helper routines of our own so that we get a feel for what's involved.  The following will be graded. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AF2keRTvk_XF"
      },
      "source": [
        "### Exercise 8.1: `count_freqs`\n",
        "Given a list, count up the number of times that each element appears in the list.  Return this as a Python dict called `freqs`:\n",
        "\n",
        "Note that this can be done as a one-liner using `Counter` from the builtin Python `collections` library, or you can write something similar from scratch yourself.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1fSC10lk_-r"
      },
      "source": [
        "## GRADED EXERCISE 8.1\n",
        "from collections import Counter\n",
        "\n",
        "def count_freqs(tokens:list):\n",
        "    ## YOUR CODE HERE\n",
        "\n",
        "    ### END OF YOUR CODE\n",
        "    return freqs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epb81-J8myaV"
      },
      "source": [
        "Here's some code to check yourself:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-D-N3bGzm0v2"
      },
      "source": [
        "test_list = ['a','b','c','a','d','z','z','q','z','b']\n",
        "freqs = count_freqs(test_list); freqs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2Ol1Cw6m45t"
      },
      "source": [
        "Expected output (note that your order may be different because dicts don't preserve order, but the values should be the same):\n",
        "```\n",
        "Counter({'a': 2, 'b': 2, 'c': 1, 'd': 1, 'q': 1, 'z': 3})\n",
        "```\n",
        "or\n",
        "```\n",
        "dict({'a': 2, 'b': 2, 'c': 1, 'd': 1, 'q': 1, 'z': 3})\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeAaBH9Jn7zC"
      },
      "source": [
        "# another test:\n",
        "assert freqs['z'] == 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMPyZukenLE1"
      },
      "source": [
        "### Exercise 8.2: `sort_by_freq`\n",
        "Given a list, sort its elements in **descending** order of frequency. You should call `count_freqs` in this function.  [Here's a hint](https://stackoverflow.com/questions/613183/how-do-i-sort-a-dictionary-by-value)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoSDkWTenVr_"
      },
      "source": [
        "## GRADED EXERCISE 8.2\n",
        "def sort_by_freq(tokens:list):\n",
        "    #call count_freqs to get the frequencies\n",
        "    freqs = ...\n",
        "\n",
        "    # then sort the tokens according to freqz\n",
        "    sorted_tokens = \n",
        "    \n",
        "    return sorted_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyExCLoPq62c"
      },
      "source": [
        "Test code for you:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiMWx75Mq8c6"
      },
      "source": [
        "assert sort_by_freq(test_list) == ['z', 'a', 'b', 'c', 'd', 'q']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3YNeIA9k-Q-"
      },
      "source": [
        "\n",
        "### Exercise 8.3: `set_vocab_codes`\n",
        "This will be akin to the \"setup\" method of fastai's Numericalize: Given an input text,...\n",
        "\n",
        "1. Tokenize it via the defined `tokenize` method. This will give you a list we'll call `tokens`. \n",
        "2. Then rank `tokens` in decreasing order of frequency of their occurance in the text.  Call your `sort_by_freq` function for this.\n",
        "3. Truncate the list of tokens and only keep the top `keep_frac` fraction of it.\n",
        "4. Add an 'xxunk' token at the beginning of the list of tokens. \n",
        "5. Finally produce a Python `dict` called `vocab_codes` that will map tokens to their index on the sorted list. \n",
        "\n",
        "Also, make sure that any unknown words applied to `vocab_codes` return as a [default dict value](https://stackoverflow.com/questions/52195897/how-to-create-a-dict-that-can-account-for-unknown-keys) the code for `xxunk`. \n",
        "\n",
        "> Note: The fastai/spacy tokenizer is setup as a *generator*, which is not helpful for this exercise. For this reason we'll use NLTK's tokenizer instead. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gekmM_NCupVw"
      },
      "source": [
        "from fastai.text.all import *\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "nltk.download('punkt')    # this is a resource needed by NLTK"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xMzagpUKoJ6"
      },
      "source": [
        "## GRADED EXERCISE 8.3\n",
        "\n",
        "def set_vocab_codes(text:string, tokenizer=nltk.word_tokenize, keep_frac=0.5):\n",
        "    # INSERT YOUR OWN CODE BELOW\n",
        "    # 1. Tokenize text via the defined `tokenizer` method. This will give you a list we'll call `tokens`.\n",
        "    tokens = ...\n",
        "\n",
        "    # 2. Then rank `tokens` in decreasing order of frequency of their occurance in the text.  Call your sort_by_freq()\n",
        "    tokens = ...\n",
        "\n",
        "    # 3. Truncate the list of tokens and only keep the top `keep_frac` fraction of it.\n",
        "    tokens = ...\n",
        "\n",
        "    # 4. Add an 'xxunk' token at the beginning of the ranked list of tokens. \n",
        "    tokens = ...\n",
        "\n",
        "    # 5. Finally produce a Python `dict` called `vocab_codes` that will map tokens to their index on the sorted list. \n",
        "    vocab_codes = ...\n",
        "\n",
        "    # Also, (You may want to do this before #5) Make sure that any unknown words applied to `vocab_codes` return as a default \n",
        "\n",
        "\n",
        "    ## END OF YOUR CODE\n",
        "    return vocab_codes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bHAWI90hQT_"
      },
      "source": [
        "text = 'The quick brown fox jumped over the lazy dog'\n",
        "codes = set_vocab_codes(text)\n",
        "codes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DZsviv0tO9I"
      },
      "source": [
        "Expected output:    Your codes dict may have a different order than this, but the values should be the same:\n",
        "\n",
        "```\n",
        "defaultdict(<function __main__.set_vocab_codes.<locals>.<lambda>>,\n",
        "            {'The': 1, 'brown': 3, 'fox': 4, 'quick': 2, 'xxunk': 0})\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNB5FRM2tMI7"
      },
      "source": [
        "# more tests for you:\n",
        "assert codes['fox'] == 4\n",
        "assert codes['Kwisatz Haderach'] == 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8LjYEA5g44n"
      },
      "source": [
        "text = 'It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair.'\n",
        "codes = set_vocab_codes(text)\n",
        "codes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dqh28Bv-tpsW"
      },
      "source": [
        "Expected output:  (Again, the dict order may not be the same, but you should see the same values)\n",
        "\n",
        "```\n",
        "defaultdict(<function __main__.set_vocab_codes.<locals>.<lambda>>,\n",
        "            {',': 4,\n",
        "             'It': 10,\n",
        "             'age': 7,\n",
        "             'best': 11,\n",
        "             'epoch': 8,\n",
        "             'it': 5,\n",
        "             'of': 3,\n",
        "             'season': 9,\n",
        "             'the': 2,\n",
        "             'times': 6,\n",
        "             'was': 1,\n",
        "             'xxunk': 0})\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HB7eMuCVt4M_"
      },
      "source": [
        "assert codes['it'] == 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_JCy5ppvmtG"
      },
      "source": [
        "You'll notice in the above example that \"It\" and \"it\" are treated as two separate words. We could send the whole text in as lowercase to get a different result:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MnFJUQavnVr"
      },
      "source": [
        "codes = set_vocab_codes(text.lower())\n",
        "codes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pb0qwdnlvqA4"
      },
      "source": [
        "Expected output: (order may not be the same)\n",
        "```\n",
        "defaultdict(<function __main__.set_vocab_codes.<locals>.<lambda>>,\n",
        "            {',': 5,\n",
        "             'age': 7,\n",
        "             'best': 10,\n",
        "             'epoch': 8,\n",
        "             'it': 1,\n",
        "             'of': 4,\n",
        "             'season': 9,\n",
        "             'the': 3,\n",
        "             'times': 6,\n",
        "             'was': 2,\n",
        "             'worst': 11,\n",
        "             'xxunk': 0})\n",
        "```\n",
        "\n",
        "...And now \"it\" is the most frequent non-unk word in the list.   We could still do like fastai and insert a 'xxmaj' code before every capitalization, but... let's move on for now.  \n",
        "\n",
        "\n",
        "## Exercise 8.4: `codes_to_words`\n",
        "One other useful thing will be a way to convert from the codes *back* to the words themselves.  Let's create a function that will return a dict in which the keys and values have been swapped.  [Here's a hint](https://www.geeksforgeeks.org/python-program-to-swap-keys-and-values-in-dictionary/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85rID2xQxfzE"
      },
      "source": [
        "## GRADED EXERCISE 8.4 \n",
        "\n",
        "def codes_to_words(codes:dict):\n",
        "    ## YOUR CODE BELOW\n",
        "    words = ...\n",
        " \n",
        "    ## END OF YOUR CODE\n",
        "    return words "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8G9YdCcxj56"
      },
      "source": [
        "words = codes_to_words(codes)\n",
        "assert words[9] == 'season'\n",
        "words[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0-j-PHwsISa"
      },
      "source": [
        "### How We Get Word Vector Embeddings\n",
        "\n",
        "It's incredibly simple: The `dict` variables that map words to codes and codes to words serve as what computational scientists \"look up tables\". Neural networks don't exactly work with look-up tables but they can work with a very close analogue via matrix multiplication: What we do is take the word codes (/indexes) an use these to denote the rows (or columns) of a *one-hot encoding*, which is a matrix with zeros almost everywhere, and a 1 in each column & row.  The simplest version being just a diagonal matrix of 1's which can be created by functions like Numpy's `eye`.\n",
        "\n",
        "> Note: Literally, the code you'd write is simply `np.eye(len(tokens))`. It's so simple that we're not even going to make a programming assignment for it, but we *will* do a slighly trickier exercise below.\n",
        "\n",
        "Say for example we only had 3 words in our vocabulary, \"James\", \"loves\", and \"tacos\".  The one-hot encoding for these could be \n",
        "\n",
        "```\n",
        "'James' = [1, 0, 0]\n",
        "'loves' = [0, 1, 0]\n",
        "'tacos' = [0, 0, 1]\n",
        "```\n",
        "\n",
        "Then a look-up table could be created when we multiply the *matrix* of one-hot encoded words (i.e.  `np.eye(3)` in this case) with whatever number we want. \n",
        "\n",
        "The \"number we want\" will be a set of numbers, in the form of *weights* of a neural network `Linear` layer (with no activation, i.e. linear activation).  This will produce our word embeddings!  To be clear: the weights will be the same as the activations because we are multiplying \"1\" by the weights and using no activation.  The dimension of the word vectors produced will be determined by how many dimensions we want in this Linear layer of weights -- for example, we mentioned 300 before.\n",
        "\n",
        "These weights (i.e. the word embeddings themselves) are initialized randomly and *learned* in the context of training the model for whatever task we want.\n",
        "\n",
        "You might wonder: is using just one Linear layer with no activation sufficient to accurately map out human language to the point fo being able to produce meaningful embeddings?  In practice, this is remarkably effective, and in fact the `nn.Embedding` layers in both PyTorch and Jupyter are literally just one-hot encoders attached to Linear layers (in Keras they're called \"Dense\" layers).  \n",
        "\n",
        "How useful will these embeddings be?  Well, that's an interesting question. \"Universal\" word embedding such as Word2Vec of GloVe are trained on huge datasets in order to be as general as possible, whereas if you were to simply train on a very small dataset, your embeddings might be only useful for the specific task you want.  Generally, it's useful to start with pretrained embeddings in a \"frozen\" (non-trainable) state as you train the downstream part of your neural network, and then gradually \"unfreeze\" the network starting fro the later layers and working backward was the model trains.  (The ULMFiT method referenced earlier describes a detailed way of doing this.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiA6hnBSsNmf"
      },
      "source": [
        "### Exercise 8.5: `token_to_one_hot`\n",
        "In this example, we're not going to encode *all* the available tokens at once, rather we're just going to produce the one-hot encodings of the particular words one might find in a sequence.  This will simply involve using the `codes` dict you created to get the index of a word and then forming a one-hot version of that word -- i.e. all zeros except for a 1 at the element corresponding to the word's index/code. The length of the one-hot vector will be the total number of possible words.  So for example, if we had 1000 words, the 'xxunk' would be one-hot encoded as a 1 in the first (0th) spot followed by a list of 999 zeros.  \n",
        "\n",
        "In the following, use `torch.zeros()` to initialize the vector, with a length of `len(codes)`.\n",
        "\n",
        "***As an additional requirement: Your routine should return the one-hot vector for 'xxunk' for any token not already assigned a code.*** (You may use recursion to achieve this if you like.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sItFtt2VsLDi"
      },
      "source": [
        "## GRADED EXERCISE 8.5\n",
        "import torch \n",
        "def token_to_one_hot(token, codes):\n",
        "    ### Your code below. Produce a 1D numpy array corresponding to the one-hot vector for token\n",
        "\n",
        "\n",
        "    ### end of your code\n",
        "    return onehot_vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoRmBwrmsQib"
      },
      "source": [
        "# a little test code for you\n",
        "from collections import defaultdict \n",
        "test_codes = defaultdict(lambda x:0) \n",
        "for key, value in {'xxunk':0, 'the':1, 'apples':2, 'are':3, 'tasty':4}.items():\n",
        "    test_codes[key] = value\n",
        "\n",
        "print(token_to_one_hot('apples', test_codes))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3kXUhbasSh0"
      },
      "source": [
        "Expected output:\n",
        "```\n",
        "    tensor([0., 0., 1., 0., 0.])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IMAmOyLsTLU"
      },
      "source": [
        "# More tests:\n",
        "assert torch.equal( token_to_one_hot('tasty',test_codes), torch.Tensor([0,0,0,0,1]) )\n",
        "\n",
        "# defaultdict will not automatically handle this next one! Your routine will need to catch it\n",
        "assert torch.equal( token_to_one_hot('smorgasbord',test_codes), torch.Tensor([1,0,0,0,0]) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1JMZgh0sW8J"
      },
      "source": [
        "Now, if our \"batch\" is going to have sequences of words, and each word is a one-hot vector, then won't we end up having a 3-dimensional array for our input?  That was fine for images because we were using 2D convolution operations.  How will we structure the one-hot encodings in our word vectors -- as rows or columns?\n",
        "\n",
        "Let's try a simple example.  Pretend these words are the tokens. We're going to want to process each word and produce its word vector. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOGADY0FsUcc"
      },
      "source": [
        "test_batch = [['here','is','a','sequence'],['and','here','is','another'],['one','more','sequence','here']]\n",
        "\n",
        "# Uhhh ok actually our set_vocab_codes needs a string, so we'll convert to an array and then flatten it\n",
        "batch_array = np.array(batch)\n",
        "print(\"batch_array = \\n\",batch_array)\n",
        "print(\"batch_array.shape =\",batch_array.shape)\n",
        "test_text = ' '.join( batch_array.flatten().tolist() )\n",
        "print(\"test_text = \",test_text)  # ok, now we've got our string to send to set_vocab_codes\n",
        "\n",
        "test_codes = set_vocab_codes( test_text )\n",
        "print(\"test_codes =\",test_codes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpOqHEAnsefe"
      },
      "source": [
        "Now we'll produce the one-hot encodings for this batch of sequences:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvsee_oysfW-"
      },
      "source": [
        "onehot_batch = torch.zeros( (batch_array.shape[0], batch_array.shape[1], len(test_codes)) )\n",
        "\n",
        "for i, row in enumerate(batch_array):\n",
        "    for j, word in enumerate(row):\n",
        "        onehot_batch[i, j] = token_to_one_hot(word, test_codes)\n",
        "\n",
        "(print(onehot_batch) # here the onehot encodings of words will appear along rows"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Vu4jmzdsgtp"
      },
      "source": [
        "(^Coulda had you do that as an exercise, but imagined it might get confusing. ;-) )\n",
        "\n",
        "That batch of inputs will then be matrix-multiplied by a set of weights in the Linear layer of a PyTorch model we'll define as we go forward. \n",
        "\n",
        "The *output* of that PyTorch model will be a single word, namely a one-hot vector for the next word in the sequence (given by our `shift_left`-ed target data. We will use `softmax` activation and a categorical cross-entropy loss function for this since it's effectively the same thing as predicting one of a variety of categories.  These are just the multi-category versions of sigmoid and binary cross-entropy we saw before. \n",
        "\n",
        "> Note: Or even better, we can get a bit more numerical precision if we *don't* use softmax and cross-entropy explicitly but rather use the PyTorch forms of these functions that will avoid any funny exponential blowups (as discussed in our [Santa Claus example](https://hedges.belmont.edu/naughty/) for binary classification). The PyTorch `nn.CrossEntropyLoss` actually expects pre-softmax \"logit\" values so we won't apply the softmax in our model.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEitxbh5skCi"
      },
      "source": [
        "#### At this point, we've covered all the moving parts of the system except for the model itself!  \n",
        "We can swap in a variety of models, but for definiteness we're going to use what's called \"Recurrent Neural Network\" (RNN) that will retain some \"memory\" of previous \"states\" when it looked at earlier sequences.  (And *this* is why we made the requirement earlier of having one batch feed directly into the next -- it's because of of this stateful memory).  \n",
        "\n",
        "The particular form of RNN we'll use is called an LSTM (which stands for \"Long Short-Term Memory\") and it has a few nice properties for not just remembering things but for \"forgetting\" things too, and even for \"deciding\" what's worth remembering or forgetting!\n",
        "\n",
        "So now, having written most of the components ourselves so we have some idea of \"what's going on under the hood,\" we'll switch over to the fastai versions of them so that we can take advantage of all the other fastai \"goodies\". \n",
        "\n",
        "> Attribution: **In what follows, we will follow a combination of [Chapter 10](https://github.com/fastai/fastbook/blob/master/10_nlp.ipynb) and [Chapter 12](https://github.com/fastai/fastbook/blob/master/12_nlp_dive.ipynb) from the fastai \"fastbook\"**, almost verbatim.  After all, *is* listed as one of the textbooks for the course. ;-)   We will however change up the order, covering the Chapter 12 content before the Chapter 10 content.\n",
        "\n",
        "What they do is, first load in a dataset and define their data loaders, then they define and train a few different RNN models ordered by increasing sophistication, finally building up to the LSTM model. (This will be the part from fastbook Chapter 12).  Then we will use this to train a language model and adapt that for text classification (this part will be from fastbook Chapter 10).\n",
        "\n",
        "Let's take a look at the simplest model they consider first. It's designed to take in three words at a time, which appear in the `.forward()` part of the class as `x[:,0]`, `x[:,1]`, and `x[:,2]`:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBDXkGv4Emk1"
      },
      "source": [
        "\n",
        "> **IMPORTANT:** For what follows, make sure you have GPU acceleration enabled.  Go to `Edit > Notebook settings > Hardware acclerator > GPU`.   This may reset your runtime, in which case you'll need to re-do the install * imports from the top of the notebook, then come back down.  If you don't have the GPU enabled, the training below will take a very long time. [link text](https://) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_5I_duFsiQE"
      },
      "source": [
        "from fastai.text.all import *\n",
        "\n",
        "class LMModel1(Module):\n",
        "    def __init__(self, vocab_sz, n_hidden):\n",
        "        self.i_h = nn.Embedding(vocab_sz, n_hidden)  \n",
        "        self.h_h = nn.Linear(n_hidden, n_hidden)     \n",
        "        self.h_o = nn.Linear(n_hidden,vocab_sz)\n",
        "        \n",
        "    def forward(self, x):  # SHH: I slightly edited this from fastbook to make it more similar to LMModel2 below\n",
        "        h = 0\n",
        "        h = h + self.i_h(x[:,0])\n",
        "        h = F.relu(self.h_h(h))\n",
        "        h = h + self.i_h(x[:,1])\n",
        "        h = F.relu(self.h_h(h))\n",
        "        h = h + self.i_h(x[:,2])\n",
        "        h = F.relu(self.h_h(h))\n",
        "        return self.h_o(h)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92whG_iu9IXb"
      },
      "source": [
        "So we've got a `nn.Embedding` layer, which as we said earlier is just a (very efficient) proxy for one-hot encoding paired with a linear layer.\n",
        "After that it's just a 3-layer network, but one in which we add the embeddings (`i_h`) of each word to the \"hidden state\" (`h`) of the network for this 3-word sequence.  This network has no \"memory\"; we'll add that later. \n",
        "\n",
        "So they setup some data and train this model, and look at the accuracy over a few epochs:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zxd3i2dNCCgh"
      },
      "source": [
        "----\n",
        "## Oct 7, 11:20pm, Pausing here.\n",
        "----\n",
        "Anything below is just a sketch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImiqQ_wW-wOm"
      },
      "source": [
        "## TODO: add data set up and model1 training"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9aP0cNw-82O"
      },
      "source": [
        "The next model they do is *exactly the same as model 1* but with the `.forward()` method just written using a loop.  Can you do it yourself (without peeking at the fastbook version)?  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezCTTXkZ_A2n"
      },
      "source": [
        "## GRADED EXERCISE 8.6\n",
        "class LMModel2(Module):\n",
        "    def __init__(self, vocab_sz, n_hidden):\n",
        "        self.i_h = nn.Embedding(vocab_sz, n_hidden)  \n",
        "        self.h_h = nn.Linear(n_hidden, n_hidden)     \n",
        "        self.h_o = nn.Linear(n_hidden,vocab_sz)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        h = 0\n",
        "        for i in range(3):\n",
        "            ### YOUR CODE BELOW: fill in the two lines needed to perform the same operations as LMModel1\n",
        "\n",
        "\n",
        "            ### END YOUR CODE.\n",
        "        return self.h_o(h)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qg3r35hCAD7O"
      },
      "source": [
        "## todo: train LMModel2 and show it's the same as LMModel1. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uxc2Nel1_R7I"
      },
      "source": [
        "NEXT we finally come to a model that has some \"memory\": the state `h` is going to persist inside the class (as `self.h`) from one `.forward()` call to the next instead of being reset to 0 every time.  They do include a method to reset the state but it has to be called explicitly, otherwise `self.h` remains whatever value it had *last time* the forward method was called."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dob9R42q_dsQ"
      },
      "source": [
        "class LMModel3(Module):\n",
        "    def __init__(self, vocab_sz, n_hidden):\n",
        "        self.i_h = nn.Embedding(vocab_sz, n_hidden)  \n",
        "        self.h_h = nn.Linear(n_hidden, n_hidden)     \n",
        "        self.h_o = nn.Linear(n_hidden,vocab_sz)\n",
        "        self.h = 0                   # this is the \"memory\" part. self.h persists with the class after .forward() is finished\n",
        "        \n",
        "    def forward(self, x):\n",
        "        for i in range(3):           # what follows is same as LMModel2 but with \"h\" replaced by \"self.h\"\n",
        "            self.h = self.h + self.i_h(x[:,i])\n",
        "            self.h = F.relu(self.h_h(self.h))\n",
        "        out = self.h_o(self.h)\n",
        "        self.h = self.h.detach()  # for next time: we'll keep the value of h but discard its extra gradient info\n",
        "        return out\n",
        "    \n",
        "    def reset(self): self.h = 0     # we'll call this at the beginning of a new set of text."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eFAlU__AQTl"
      },
      "source": [
        "(some extra text and such....  then...)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcZroFzLAUjA"
      },
      "source": [
        "# train this model and show it's better than the previous model(s)! Yay.\n",
        "# note: this is still not at LSTM. "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbz2m3P4AdEn"
      },
      "source": [
        "wow, that was great.  Should we stop there?\n",
        "\n",
        "discussion of \"create more signal\".. \n",
        "\n",
        "longer sequences:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBt10YEMJ-4J"
      },
      "source": [
        "## add some data loading code whereby we build batches with longer sequences\n",
        "sl = 16    # sequence length\n",
        "### and stuff"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Eh3-bjyKSlW"
      },
      "source": [
        "...and a new model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q525bzenAYvn"
      },
      "source": [
        "class LMModel4(Module):\n",
        "    def __init__(self, vocab_sz, n_hidden):\n",
        "        self.i_h = nn.Embedding(vocab_sz, n_hidden)  \n",
        "        self.h_h = nn.Linear(n_hidden, n_hidden)     \n",
        "        self.h_o = nn.Linear(n_hidden,vocab_sz)\n",
        "        self.h = 0\n",
        "        \n",
        "    def forward(self, x):\n",
        "        outs = []\n",
        "        for i in range(sl):   # Note: sl (sequence length) is a global variable defined above\n",
        "            self.h = self.h + self.i_h(x[:,i])\n",
        "            self.h = F.relu(self.h_h(self.h))\n",
        "            outs.append(self.h_o(self.h))\n",
        "        self.h = self.h.detach()\n",
        "        return torch.stack(outs, dim=1)\n",
        "    \n",
        "    def reset(self): self.h = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZJXSrFiAhns"
      },
      "source": [
        "# and train the model"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRfl0VNMAj88"
      },
      "source": [
        "...oooh and that's even better, but can we still improve it? \n",
        "\n",
        "...discuss...\n",
        "\n",
        "Let's use PyTorch's RNN class to help us make an even deeper model, where we can specify the depth via `n_layers`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1enycl9zAie6"
      },
      "source": [
        "class LMModel5(Module):\n",
        "    def __init__(self, vocab_sz, n_hidden, n_layers):\n",
        "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
        "        self.rnn = nn.RNN(n_hidden, n_hidden, n_layers, batch_first=True)\n",
        "        self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
        "        self.h = torch.zeros(n_layers, bs, n_hidden)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        res,h = self.rnn(self.i_h(x), self.h)\n",
        "        self.h = h.detach()\n",
        "        return self.h_o(res)\n",
        "    \n",
        "    def reset(self): self.h.zero_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JkFpWZ3AzIC"
      },
      "source": [
        "# ...and train a deep RNN..."
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw9U-XHcAvoT"
      },
      "source": [
        "Wait, that was actually WORSE this time?  How come? Isn't deeper better?\n",
        "\n",
        "...talk about gradients & activations exploding or disappearing...\n",
        "\n",
        "And finally we come to... \n",
        "\n",
        "## LSTM  \n",
        "\n",
        "(picture)\n",
        "\n",
        "Looks scary. The parts are really just \"tunable switches\", implemented using tanh & sigmoid.\n",
        "\n",
        "This is just one LSTM \"cell\", kind of like one neuron. Here's the code for it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tipxfxMJA7eY"
      },
      "source": [
        "class LSTMCell(Module):\n",
        "    def __init__(self, ni, nh):\n",
        "        self.ih = nn.Linear(ni,4*nh)\n",
        "        self.hh = nn.Linear(nh,4*nh)\n",
        "\n",
        "    def forward(self, input, state):\n",
        "        h,c = state\n",
        "        # One big multiplication for all the gates is better than 4 smaller ones\n",
        "        gates = (self.ih(input) + self.hh(h)).chunk(4, 1)\n",
        "        ingate,forgetgate,outgate = map(torch.sigmoid, gates[:3])\n",
        "        cellgate = gates[3].tanh()\n",
        "\n",
        "        c = (forgetgate*c) + (ingate*cellgate)\n",
        "        h = outgate * c.tanh()\n",
        "        return h, (h,c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8slav4CBRoN"
      },
      "source": [
        "We then chain these cells together. Here's a model that does that:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20vQkphdBTPW"
      },
      "source": [
        "class LMModel6(Module):\n",
        "    def __init__(self, vocab_sz, n_hidden, n_layers):\n",
        "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
        "        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)\n",
        "        self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
        "        self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n",
        "        \n",
        "    def forward(self, x):\n",
        "        res,h = self.rnn(self.i_h(x), self.h)\n",
        "        self.h = [h_.detach() for h_ in h]\n",
        "        return self.h_o(res)\n",
        "    \n",
        "    def reset(self): \n",
        "        for h in self.h: h.zero_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-e5HofHBYLC"
      },
      "source": [
        "# and train that..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esssv4S0BZSi"
      },
      "source": [
        "Wonderful! that's the best so far!\n",
        "\n",
        "but, can we still do better?\n",
        "\n",
        "\n",
        "regularlization via dropout\n",
        "\n",
        "which yields our final model of the day:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2zN05hGBirh"
      },
      "source": [
        "class LMModel7(Module):\n",
        "    def __init__(self, vocab_sz, n_hidden, n_layers, p):\n",
        "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
        "        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)\n",
        "        self.drop = nn.Dropout(p)\n",
        "        self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
        "        self.h_o.weight = self.i_h.weight\n",
        "        self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n",
        "        \n",
        "    def forward(self, x):\n",
        "        raw,h = self.rnn(self.i_h(x), self.h)\n",
        "        out = self.drop(raw)\n",
        "        self.h = [h_.detach() for h_ in h]\n",
        "        return self.h_o(out),raw,out\n",
        "    \n",
        "    def reset(self): \n",
        "        for h in self.h: h.zero_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5os9xo7Bo12"
      },
      "source": [
        "# and train..."
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0kNL1aaBnXP"
      },
      "source": [
        "...oh yeah. that's totally awesome!\n",
        "\n",
        "\n",
        "## NOW WHAT?\n",
        "\n",
        "Now that we've trained a LM, we'll cut off its head and use its body for a new model aimed at classifying text. (Chapter 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjkpD5MsBXxp"
      },
      "source": [
        ""
      ]
    }
  ]
}