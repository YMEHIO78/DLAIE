{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "8_DeeperNLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmeJzNsGP41j"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1blkWFKMYqinrtTSKWO_Uvv7v_TvtX6zs?usp=sharing)\n",
        "\n",
        "# Lesson 8: Going Deeper into NLP\n",
        "\n",
        "Previously we saw how convenient it was to use the `pipeline` method of the HuggingFace.co `transformers` library to perform a variety of Natural Language Processing (NLP) tasks. But there's a lot going on under the hood that was hidden from us.  If we want to learn how these models work, we're going to have to peel back several layers, on multiple levels.  \n",
        "\n",
        "What we did in the previous lesson was a bit like watching a big rocket take off from a distance. There are many systems in the rocket that are all working together to effect the launch.  To understand how the big rocket operates, it will help if we go back to study smaller, simpler rockets so that we understand the principles of rocketry. \n",
        "\n",
        "In this lesson we'll learn the parts of an NLP model and see how they go together. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGWoRTrURumS"
      },
      "source": [
        "## 1. Tokenization\n",
        "\n",
        "Whatever NLP task we're interested in performing, there will be a large amount of text (sometimes called a \"corpus\") that we will use for training the model on. That text needs to be split up somehow into bite-sized parts to operate upon. This process is known as *tokenization*. We could try treating individual characters as tokens, or [regard entire sentences as our tokens](https://claritynlp.readthedocs.io/en/latest/developer_guide/algorithms/sentence_tokenization.html), but a common mid-point is to use *words* as tokens.  \n",
        "\n",
        "> *For a great example of a character-based neural network, see [Andrej Karpaty's Char-RNN](https://github.com/karpathy/char-rnn).\n",
        "\n",
        "The simplest -- and typically *the default* -- scheme for word-level tokenization is just to split the text at every space and at every punctuation mark. Let's try an example\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQyxw7I4zpJf"
      },
      "source": [
        "So for instance.\n",
        "```\n",
        "I'm going to the store, because I need some milk.\n",
        "```\n",
        "might become\n",
        "```\n",
        "[\"I\", \"'\", \"m\", \"going\", \"to\", \"the\",  \"store\", \",\", \"because\", \"I\", \"need\", \"some\", \"milk\", \".\"]\n",
        "```\n",
        "Tokenization is something that many computational linguists have spent a great deal of time on, and there are [a variety of tokenizers](https://towardsdatascience.com/overview-of-nlp-tokenization-algorithms-c41a7d5ec4f9?gi=73a2ec14356e) available. Generally it's generally in our best interest to just call a library such as[Natural Language Toolkit (NLTK)](https://www.nltk.org/) to do the tokenizing for us instead of trying to do it from scratch. Both FastAI and HuggingFace allow us to choose between a variety of tokenizers.  (FastAI's default tokenizer is currently from the [spaCy NLP library](https://spacy.io/).)\n",
        "\n",
        "Let's try an actual example using the NLTK word tokenizer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVfwpXS703Os",
        "outputId": "ee11c893-03a9-4173-f4dc-ad2b805eb1e2"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')    # this is a resource needed by NLTK\n",
        "sentence = \"I'm going to the store, because I need some milk.\"\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "['I', \"'m\", 'going', 'to', 'the', 'store', ',', 'because', 'I', 'need', 'some', 'milk', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSr3MFM208sK"
      },
      "source": [
        "Interesting that the apostrophe from \"I'm\" went with the \"m\" (as in \"'m\") instead of being its own thing. Presumably this is so we can then expand it into \"am\".  What about the \"n\" in \"don't\"? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWotEHBA1Onf",
        "outputId": "8af10c7b-20d4-447c-8aeb-40cac0f2fe11"
      },
      "source": [
        "sentence2 = \"I don't know what's going to happen in this case, but it should be interesting!\"\n",
        "tokens = nltk.word_tokenize(sentence2)\n",
        "print(tokens)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'do', \"n't\", 'know', 'what', \"'s\", 'going', 'to', 'happen', 'in', 'this', 'case', ',', 'but', 'it', 'should', 'be', 'interesting', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVcdMXA01U7F"
      },
      "source": [
        "Ok, so in that case the \"n\" from \"don't\" went with the \"'t\". Again, this best facilitates filling in the missing \"o\".  Let's try some spirited Tennessee-style language:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGrenE9u1hcd",
        "outputId": "9befdbe3-9370-4d68-d286-53ec303b1012"
      },
      "source": [
        "sentence3 = \"I'm fixin' to spend $1499.95 on a new four wheeler and you ain't gonna stop me, ma!\"\n",
        "nltk.word_tokenize(sentence3)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " \"'m\",\n",
              " 'fixin',\n",
              " \"'\",\n",
              " 'to',\n",
              " 'spend',\n",
              " '$',\n",
              " '1499.95',\n",
              " 'on',\n",
              " 'a',\n",
              " 'new',\n",
              " 'four',\n",
              " 'wheeler',\n",
              " 'and',\n",
              " 'you',\n",
              " 'ai',\n",
              " \"n't\",\n",
              " 'gon',\n",
              " 'na',\n",
              " 'stop',\n",
              " 'me',\n",
              " ',',\n",
              " 'ma',\n",
              " '!']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raD2F2CE2K92"
      },
      "source": [
        "Wow, it knows \"you'uns\"!  And it splits \"gonna\", presumably in preparation for a mapping to \"going\", \"to\".\n",
        "\n",
        "Do we need the commas and exclamation points though?  Maybe, maybe not.  It depends on our use case.  Sometimes other punctuation is relevant, such as hashtags and @-symbols for social media.  NLTK has a special tokenizer for Twitter:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ib1dPC01j24G",
        "outputId": "88691815-a67b-497c-ceb7-80cbeda5fd25"
      },
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "tt = TweetTokenizer()\n",
        "\n",
        "tweet = \"OMG I love @SuperFamousPerson's new look! #fridays #nofilter\"\n",
        "\n",
        "# Let's compare the two tokenizers:\n",
        "print(\"Regular word tokenizer:\", nltk.word_tokenize(tweet))\n",
        "print(\"Tweet tokenizer:       \",tt.tokenize(tweet))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regular word tokenizer: ['OMG', 'I', 'love', '@', 'SuperFamousPerson', \"'s\", 'new', 'look', '!', '#', 'fridays', '#', 'nofilter']\n",
            "Tweet tokenizer:        ['OMG', 'I', 'love', '@SuperFamousPerson', \"'\", 's', 'new', 'look', '!', '#fridays', '#nofilter']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVd1JdCSk5mg"
      },
      "source": [
        "...So the specialty `TweetTokenizer` kept certain kinds of punctuation with their associated words, rather then splitting at all forms of punctuation like the regular word tokenizer did."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqXeh85J02ZM"
      },
      "source": [
        "\n",
        "\n",
        "Beyond the question of which punctuation to keep, we must also recognize that words come in a variety of forms.  And some words may be \"filler\" that we may not need for the task at hand (e.g., articles like \"a\", \"an\", and \"the\" are often discarded).  So we may wish to regard related words such as \"jump\", \"jumping\", \"jumps\",... as variations on the *stem* of \"jump\".  We may hang on to the endings such as \"-ing\" for later use, regarding them as additional tokens. The process of *stemming* or \"*stemmification*\" is the breaking up of words into their stems and hanging on to endings (or not).  Also, what about compound words?  Some languages such as German will make very long single words (e.g. Geschwindigkeitsbegrenzung for \"speed limit\") that in other languages would be considered as separate words. If language translation is our goal, some way of tokenizing that includes such variability would be important.  Also, what about punctuation? To keep things simple, we could just delete all forms of punctuation -- or expand contractions like \"I'll\" to \"I will\", and so forth -- and yet if we want a highly accurate model we may find that holding on to some forms of punctuation will important.\n",
        "\n",
        "\n",
        "\n",
        "####  Special Token Codes\n",
        "Often language models will make use of special tokens such as `UNK` (a token to substitute for unknown words) or `PAD` (for extra padding words), or `EOS` (end of sentence), depending on the task at hand. Sometimes these will have extra characters like `<UNK>` or `[UNK]`. There may or may not be `<START>` and `<END>` tokens for the beginning and end of the text.  The exact list of special tokens depends on the tokenizer and the model, but those few are pretty universal. So when you see those, in what follows, you'll be prepared.  \n",
        " \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5K6M4aTmWLW"
      },
      "source": [
        "## Numericalization / Word Vectors\n",
        "Once we have the tokens, we still need to convert these into numbers somehow so we can operate on them mathematically. Depending on the application, different numericalization schemes are available. \n",
        "\n",
        "One *very simple* way to do this if we were, say, doing *Sentiment Classification* in tweets, movie reviews, or other kinds of \"posts\",  would be to count the frequency of all the words that appear in positive posts, and do the same for all the negative posts.  Expressing these frequencies as fractions of the total number of words, we could then assign to each word its pair of \"positive use\" and \"negative use\" frequency values $(f_p, f_n)$ which lie in the two-dimensional [unit square](https://en.wikipedia.org/wiki/Unit_square) (shown below). These would then form the coordinates for a *word vector* of our word in its *embedding* space (i.e., the unit square in this case).  Then to classify a post, we could just take the sum of the word vectors of all the words in the post and see whether the result is more \"positive\" than \"negative\". In other words, we could ask, which region of the following embedding diagram does the mean of the word vectors in the post lie in?\n",
        "\n",
        "![img of regions of positive and negative](https://i.imgur.com/mLpQHBj.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqpcM4r6tpO-"
      },
      "source": [
        "That might suffice as a simple baseline model, and it can work \"ok\", but there are issues with it. For example, it's possible that different words could get mapped to the exact same point. If all you care about is how positive or how negative the post (or tweet, or review) is, this may not be a problem,  but if you want to \"understand\" the text, produce a translation of it, or generate new text, then this method is useless. \n",
        "\n",
        "> Terminology: that this method of just summing up the word vectors together pays no attention to the *order* of the words, so the above model would be termed a \"Bag of Words\" model.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfFS04zavBVD"
      },
      "source": [
        "In order to help preserve uniqueness as well as to better allow words to express their ranges of meanings, one typically uses many more than two dimensions for word vector embeddings.  It's quite common to see 256 or more (e.g. 300) dimensions for words.  While these are too many dimensions to visualize (which is why I gave the simple example above!) the computer is able to deal with them just fine.  \n",
        "\n",
        "The way one typically gets these word vectors is to take in the list of all the (unique) words in the corpus and produce a \"vocabulary\" which indexes the words and generates a one-hot encoding by treating the words as categories.  Then we map these categories into word vectors via a matrix of trainable weights. So, for example, a corpus with 10,000 unique words mapped into 300-dimensional word vectors would involve a weights matrix of 300\\*10000 = 3 million weights.  Thus *the \"embedding\" mapping is itself a neural network* which we train as the front-end of our full (larger) neural network.\n",
        "This means that the more words you allow in your vocabulary (or \"vocab\"), the bigger that initial embedding operation will be.  Typically, in order to keep this matrix from getting too big, one will truncate the list of words by removing the less frequent or less important words from the vocab and replacing them with special tokens such as `UNK`. \n",
        "The form the embedding takes may depend on the task.  \n",
        "\n",
        "## Language Modeling as a Pretraining Task\n",
        "One very useful method is to use a *language model* task to produce word embeddings.  A language model tries to predict the next word in a sequence given its preceding words (how many preceding words you use determines the sophistiation of the model). This forms a \"self-supervised learning\" method in the sense that the target data you train on is the same as the input data, just shifted ahead by one word. \n",
        "\n",
        "This approach was used to great effect by Jeremy Howard and Sebastian Ruder in their [ULMFit paper](https://paperswithcode.com/method/ulmfit), in which they used a language model task of predicting the next word in Wikipedia (specifically, the [Wikitext-103](https://paperswithcode.com/dataset/wikitext-103) dataset) in order to condition the model to use for other tasks such as sentiment analysis of IMDB movie reviews.  Their result was that they beat other competing sentiment analysis methods by a longshot!  \n",
        "\n",
        "The idea is that a model that has to predict the next word in a large text has to develop somewhat of an \"understanding\" of how language works, and thus will be a more powerful model for text classification than a simpler model that \n",
        "\n",
        "> Note: A neat effect of this form of pre-training is that you also end up with a text generation model.\n",
        "\n",
        "Now, we're not going to train a model on Wikipedia right now.  That would be a waste of time, as we can just download pretrained weights and go from there.  Let's use the fastai set of methods for doing this, and we'll work through their IMDB example problem [as described in Chapter 10 of the `fastbook`](https://github.com/fastai/fastbook/blob/master/10_nlp.ipynb).  To get started we'll need to download the dataset and start using fastai's tokenizer(s)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEjn2JVGP0Ms"
      },
      "source": [
        "!pip install -Uqq fastai fastbook"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLEk7zgv41Uf"
      },
      "source": [
        "# if the next line produces an error, restart the runtime and try again.\n",
        "import fastbook  \n",
        "from fastai.text.all import *\n",
        "from IPython.display import display, HTML"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIUqbBRN4nRB"
      },
      "source": [
        " path = untar_data(URLs.IMDB)  # download the dataset"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lp0X73dm4X3R",
        "outputId": "b40e1ab7-ef2c-4414-9c64-1992c9d526f8"
      },
      "source": [
        "# make a list of all the files in all the folders of the dataset\n",
        "files = get_text_files(path, folders = ['train', 'test', 'unsup'])\n",
        "\n",
        "# let's look at the first 75 characters of the first file in the list\n",
        "txt = files[0].open().read();  txt[:75]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Kannathil Muthamittal is for sure a great movie. I have to give it to Mani '"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcU9CTZD6hG8"
      },
      "source": [
        "As we mentioned above the current default tokenizer in FastAI is from the spaCy NLP package:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGshRMNu6bzS",
        "outputId": "96db2534-70a1-413b-988c-39c5bbfe9e21"
      },
      "source": [
        "spacy = WordTokenizer()\n",
        "spacified = spacy([txt])  \n",
        "print(spacified)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<generator object SpacyTokenizer.__call__.<locals>.<genexpr> at 0x7f9f15d77f50>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeCAdqIn7K6u"
      },
      "source": [
        "So the word tokenizer is a generator. In order to access its output we can use `first()` and `next()`:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrcspiSf6pUx",
        "outputId": "782ecd8d-94e6-4e4d-a47b-7be714bb2167"
      },
      "source": [
        "toks = first(spacy([txt]))\n",
        "print(toks)   # This prints out all the tokens\n",
        "print(coll_repr(toks, 30))  # fastai's coll_repr method gives the total size and first N (=30) tokens"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Kannathil', 'Muthamittal', 'is', 'for', 'sure', 'a', 'great', 'movie', '.', 'I', 'have', 'to', 'give', 'it', 'to', 'Mani', 'Ratnam', 'for', 'a', 'great', 'directing', 'job', 'and', 'A.R.', 'Rahman', 'for', 'great', 'songs', '.', 'The', 'camera', 'work', 'is', 'just', 'excellent', 'and', 'is', 'similar', 'to', 'Black', 'Hawk', 'Down', 'and', 'Saving', 'Private', 'Ryan', '.', 'I', 'will', 'be', 'shocked', 'if', 'this', 'movie', 'does', 'not', 'win', 'an', 'Oscar', 'for', 'Best', 'Foreign', 'Film', 'or', 'even', 'Best', 'Camera', 'Work', '.']\n",
            "(#69) ['Kannathil','Muthamittal','is','for','sure','a','great','movie','.','I','have','to','give','it','to','Mani','Ratnam','for','a','great','directing','job','and','A.R.','Rahman','for','great','songs','.','The'...]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOcUtcWh8Gtp"
      },
      "source": [
        "In addition to `WordTokenizer`, fastai adds some extra functionality via a `Tokenizer` method, that will turn all words to lower case but precede such interventions with a special code `xxmaj` indicating that the next word should be capitalized.  It also adds `xxbos` to denote the beginning of the sentence. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_k9AH3Sj761H",
        "outputId": "134f7ab1-82e0-4582-9b27-f2309457f473"
      },
      "source": [
        "tkn = Tokenizer(spacy)\n",
        "print(coll_repr(tkn(txt), 31))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(#91) ['xxbos','xxmaj','kannathil','xxmaj','muthamittal','is','for','sure','a','great','movie','.','i','have','to','give','it','to','xxmaj','mani','xxmaj','ratnam','for','a','great','directing','job','and','xxup','a.r','.'...]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5wJRkJ18trP"
      },
      "source": [
        "> Note: fastai also has a tokenization method that will use sub-words -- i.e., groups of characters -- but we're going to skip that part for now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfZq-2xc89AS"
      },
      "source": [
        "To do calculations on the GPU, it's helpful to work with \"batches\" of data, just like we did for images.  In each batch we need the same demensions, so we will chop the text up into \"chunks\" of length `seq_len` and then group these into batches.  Rather than totally randomly assigning the order of the batches, we will have the model \"read\" the text sequentially, where each new element of a batch will simply be shifted ahead one word. \n",
        "\n",
        "See this fastai example where they use a batch size of `bs=6` and sequence length of `seq_len=5` to produce one batch from a sample text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbhnhqg3-X0M",
        "outputId": "e8f5766d-d011-46b2-c9d3-5591ab175e6f"
      },
      "source": [
        "stream = \"In this chapter, we will go back over the example of classifying movie reviews we studied in chapter 1 and dig deeper under the surface. First we will look at the processing steps necessary to convert text into numbers and how to customize it. By doing this, we'll have another example of the PreProcessor used in the data block API.\\nThen we will study how we build a language model and train it for a while.\"\n",
        "print(stream)\n",
        "tokens = tkn(stream)\n",
        "print(\"\\n\",len(tokens),\"tokens in stream.\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In this chapter, we will go back over the example of classifying movie reviews we studied in chapter 1 and dig deeper under the surface. First we will look at the processing steps necessary to convert text into numbers and how to customize it. By doing this, we'll have another example of the PreProcessor used in the data block API.\n",
            "Then we will study how we build a language model and train it for a while.\n",
            "\n",
            " 90 tokens in stream.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VB_E9QMY_Y29"
      },
      "source": [
        "Although we could randomly grab \"chunks\" from all over the file and try to predict the word following each chunk, the fastai folks recommend making the text in each row of each batch follow immediately from the text in the corresponding row the previous batch.  Which means making some fancy slicing code like the following, in which we show three sequential batches.   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "id": "yMiGHdF67-ab",
        "outputId": "02db36f8-ad8f-4982-def4-70a9cb9ba99e"
      },
      "source": [
        "\n",
        "bs,seq_len = 6, 5                          # batch size and sequence length\n",
        "num_batches = len(tokens)// bs // seq_len  # 30 tokens per batch, 90 tokens = 3 batches. \n",
        "print(\"num_batches = \",num_batches)  \n",
        "num_rows = len(tokens) // seq_len          # total rows of all batches == 18\n",
        "print(\"num_rows = \",num_rows)\n",
        "\n",
        "for b in range(num_batches):\n",
        "    stride = seq_len * num_batches \n",
        "    d_tokens = np.array([tokens[i*stride + b*seq_len :i*stride + b*seq_len + seq_len] for i in range(bs)]) # i is the row number\n",
        "    df = pd.DataFrame(d_tokens)\n",
        "    print(f\"\\nbatch = {b}:\")\n",
        "    display(HTML(df.to_html(index=False,header=None)))\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_batches =  3\n",
            "num_rows =  18\n",
            "\n",
            "batch = 0:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>xxbos</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>in</td>\n",
              "      <td>this</td>\n",
              "      <td>chapter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>movie</td>\n",
              "      <td>reviews</td>\n",
              "      <td>we</td>\n",
              "      <td>studied</td>\n",
              "      <td>in</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>first</td>\n",
              "      <td>we</td>\n",
              "      <td>will</td>\n",
              "      <td>look</td>\n",
              "      <td>at</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>how</td>\n",
              "      <td>to</td>\n",
              "      <td>customize</td>\n",
              "      <td>it</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>of</td>\n",
              "      <td>the</td>\n",
              "      <td>preprocessor</td>\n",
              "      <td>used</td>\n",
              "      <td>in</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>will</td>\n",
              "      <td>study</td>\n",
              "      <td>how</td>\n",
              "      <td>we</td>\n",
              "      <td>build</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "batch = 1:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>,</td>\n",
              "      <td>we</td>\n",
              "      <td>will</td>\n",
              "      <td>go</td>\n",
              "      <td>back</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>chapter</td>\n",
              "      <td>1</td>\n",
              "      <td>and</td>\n",
              "      <td>dig</td>\n",
              "      <td>deeper</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>the</td>\n",
              "      <td>processing</td>\n",
              "      <td>steps</td>\n",
              "      <td>necessary</td>\n",
              "      <td>to</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>xxmaj</td>\n",
              "      <td>by</td>\n",
              "      <td>doing</td>\n",
              "      <td>this</td>\n",
              "      <td>,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>the</td>\n",
              "      <td>data</td>\n",
              "      <td>block</td>\n",
              "      <td>xxup</td>\n",
              "      <td>api</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>a</td>\n",
              "      <td>language</td>\n",
              "      <td>model</td>\n",
              "      <td>and</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "batch = 2:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>over</td>\n",
              "      <td>the</td>\n",
              "      <td>example</td>\n",
              "      <td>of</td>\n",
              "      <td>classifying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>under</td>\n",
              "      <td>the</td>\n",
              "      <td>surface</td>\n",
              "      <td>.</td>\n",
              "      <td>xxmaj</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>convert</td>\n",
              "      <td>text</td>\n",
              "      <td>into</td>\n",
              "      <td>numbers</td>\n",
              "      <td>and</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>we</td>\n",
              "      <td>'ll</td>\n",
              "      <td>have</td>\n",
              "      <td>another</td>\n",
              "      <td>example</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>.</td>\n",
              "      <td>\\n</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>then</td>\n",
              "      <td>we</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>it</td>\n",
              "      <td>for</td>\n",
              "      <td>a</td>\n",
              "      <td>while</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aE80zhFGFeXr"
      },
      "source": [
        "See how each row of each batch continues the text from the same row in the preceding batch? \n",
        "\n",
        "When we were training images, we shuffled the order of images between epochs.  In the case of NLP we don't want to shuffle the words or even the rows.  Instead when we take a bunch of movie reviews and concatenate them to form a stream (which then broken into tokens and then batches), what we do is randomize the *order in which the reviews are concatenated* at each epoch.  This allows for word orderings to stay the same but where they appear in the training dataset to still shift around a bit in order to prevent overfitting. \n",
        "\n",
        "\n",
        "This is generally handled automatically by fastai, that will define the Tokenizer, set it up, and specify a Numericalize function, and set that up.  Here we show a brief example of that:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "D4SGCUpz9JrL",
        "outputId": "81e490a7-f02d-4bf6-ac73-b2b3862b5077"
      },
      "source": [
        "txts = L(o.open().read() for o in files[:2000])  # read texts of the first 2000 files\n",
        "txts[0]"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Kannathil Muthamittal is for sure a great movie. I have to give it to Mani Ratnam for a great directing job and A.R. Rahman for great songs. The camera work is just excellent and is similar to Black Hawk Down and Saving Private Ryan. I will be shocked if this movie does not win an Oscar for Best Foreign Film or even Best Camera Work.'"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d1ngrlbGvEv",
        "outputId": "f76d6c75-dff5-4484-b879-22d5c2097b1c"
      },
      "source": [
        "toks200 = txts[:200].map(tkn)   # tokenize the first 200 files, by mapping the \"tkn\" function to the elements of text.\n",
        "toks200[0]  # show us the tokens corresponding to the text in the first file "
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#91) ['xxbos','xxmaj','kannathil','xxmaj','muthamittal','is','for','sure','a','great'...]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "YBF0Z62qHJ3V",
        "outputId": "7886fee0-2ee5-4548-f185-5961259a98e1"
      },
      "source": [
        "num = Numericalize()\n",
        "num.setup(toks200)   # create a vocab for the stream we've created. \n",
        "coll_repr(num.vocab,20)  # show the first 20 words in the vocab, in order of descending frequency"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"(#1904) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj','the',',','.','and','a','of','to','is','in','it','i'...]\""
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DA4kJ85tHpJL"
      },
      "source": [
        "Then we can show how these individual tokens are rendered as numbers.  Note that the special codes get mapped to zero:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1QT3GCsHWYV",
        "outputId": "8d3945f3-524d-4aa4-d3be-e872c136bf30"
      },
      "source": [
        "nums200 = toks200.map(num);\n",
        "print(toks200[0])\n",
        "print(nums200[0])"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['xxbos', 'xxmaj', 'kannathil', 'xxmaj', 'muthamittal', 'is', 'for', 'sure', 'a', 'great', 'movie', '.', 'i', 'have', 'to', 'give', 'it', 'to', 'xxmaj', 'mani', 'xxmaj', 'ratnam', 'for', 'a', 'great', 'directing', 'job', 'and', 'xxup', 'a.r', '.', 'xxmaj', 'rahman', 'for', 'great', 'songs', '.', 'xxmaj', 'the', 'camera', 'work', 'is', 'just', 'excellent', 'and', 'is', 'similar', 'to', 'xxmaj', 'black', 'xxmaj', 'hawk', 'xxmaj', 'down', 'and', 'xxmaj', 'saving', 'xxmaj', 'private', 'xxmaj', 'ryan', '.', 'i', 'will', 'be', 'shocked', 'if', 'this', 'movie', 'does', 'not', 'win', 'an', 'xxmaj', 'oscar', 'for', 'xxmaj', 'best', 'xxmaj', 'foreign', 'xxmaj', 'film', 'or', 'even', 'xxmaj', 'best', 'xxmaj', 'camera', 'xxmaj', 'work', '.']\n",
            "TensorText([   2,    8,    0,    8,    0,   16,   30,  273,   13,   72,   29,   11,   19,   45,   15,  223,   18,   15,    8,    0,    8,    0,   30,   13,   72,  469,  297,   12,    7,    0,   11,    8,\n",
            "           0,   30,   72,  470,   11,    8,    9,  426,  164,   16,   70,  191,   12,   16,  471,   15,    8,  298,    8,    0,    8,  170,   12,    8,    0,    8,    0,    8, 1069,   11,   19,  101,\n",
            "          46,    0,   74,   20,   29,   88,   40,  650,   49,    8,  581,   30,    8,   84,    8,    0,    8,   26,   59,  118,    8,   84,    8,  426,    8,  164,   11])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teZw3UP9H_Bo"
      },
      "source": [
        "^Note how the unknown / low frequency words get mapped to 0. \n",
        "\n",
        "These can then go into a fastai DataLoader which has been setup for language modeling, [`LMDataLoader`](https://docs.fast.ai/text.data.html#LMDataLoader), which is designed to load a batch of text as an input and the *same text shifted ahead by one word* as the target data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oE8oEJW8Hnu2",
        "outputId": "031f1fbd-004c-40b1-c2fa-eb9ac7c5358e"
      },
      "source": [
        "dl = LMDataLoader(nums200)\n",
        "\n",
        "# test it\n",
        "x,y = first(dl)\n",
        "print(x.shape,y.shape)\n",
        "\n",
        "# we can print out x & y but lets convert them from numbers to text when we view them\n",
        "print(', '.join(num.vocab[o] for o in x[0][:20]))\n",
        "print(', '.join(num.vocab[o] for o in y[0][:20]))"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 72]) torch.Size([64, 72])\n",
            "xxbos, xxmaj, xxunk, xxmaj, xxunk, is, for, sure, a, great, movie, ., i, have, to, give, it, to, xxmaj, xxunk\n",
            "xxmaj, xxunk, xxmaj, xxunk, is, for, sure, a, great, movie, ., i, have, to, give, it, to, xxmaj, xxunk, xxmaj\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RM15JYsCJWTX"
      },
      "source": [
        "See how each word in y is just the corresponding \"next word in x\" at the same index?  As a simple exercise, can you do the same?  Write a \"shift left\" function that just shifts a set of list elements to the left.  Add a \"xxpad\" on the end:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lCCCSuJIldm"
      },
      "source": [
        "## EXERCISE. Fill in your code below as directed\n",
        "\n",
        "def shift_left(orig:list):   \n",
        "    ## Your code below. Define a variable called \"shifted\" that is the original \n",
        "    #  list, shifted to the left by one, and filled in with a \"xxpad\" at the end.\n",
        " \n",
        "    shifted =  \n",
        " \n",
        "    ## end of your code\n",
        "    return shifted "
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvAPBosZKsKE"
      },
      "source": [
        "Test your code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIYzPWbGKcQV",
        "outputId": "1dda733d-fb74-49fd-ad09-83cac5ef31f8"
      },
      "source": [
        "shift_left([1,2,3,4,5])"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 3, 4, 5, 'xxpad']"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hg427WeUKiEV"
      },
      "source": [
        "```\n",
        "Expected ouput:\n",
        "[2, 3, 4, 5, 'xxpad']\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omqK89l_KgJl"
      },
      "source": [
        "# and another check\n",
        "assert shift_left([]) == ['xxpad']"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7slem8hLVod"
      },
      "source": [
        "## Pausing Here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xMzagpUKoJ6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}