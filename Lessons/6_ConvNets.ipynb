{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6_ConvNets.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8STqvwWtosEm"
      },
      "source": [
        "# Convolutional Networks\n",
        "\n",
        "We've talked about the \"MLP\" architecture in previous lessons, in which inputs to the next layer of neurons are \"fully connected\" (everything is conntected to everything else). \n",
        "\n",
        "It turns out that for many applications in computer vision and signal processing, one can make great use of layers that look at only \"local\" (or \"nearby\") information, and then have the outpouts of those be combined at successively larger spatial (or temporal) scales.  This is what's known as a *local receptive field*, and is based directly on the biology of neurons in the visual cortex:\n",
        "\n",
        "![neuron receptive field](https://i.imgur.com/KC4PsPc.png)\n",
        "(Image source: http://neuroclusterbrain.com/neuron_model.html)\n",
        "\n",
        "This idea was exploided in the [\"Neocognitron\" model](https://link.springer.com/article/10.1007/BF00344251) which built off the Nobel-prize winning work of Hubel & Weisel with cat vision, and how the different neurons in the cat brain learn to function as \"pattern recognizers\" for things like edges:\n",
        "\n",
        "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/IOHayh06LJ4\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "5kN1ZYerpRFn",
        "outputId": "9b53d184-6338-4ac3-dd2d-e7bab72d55d0"
      },
      "source": [
        "from IPython.display import YouTubeVideo\n",
        "YouTubeVideo('IOHayh06LJ4')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"400\"\n",
              "            height=\"300\"\n",
              "            src=\"https://www.youtube.com/embed/IOHayh06LJ4\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.YouTubeVideo at 0x7f4117f26a90>"
            ],
            "image/jpeg": "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\n"
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nm3rp0qCpNdc"
      },
      "source": [
        "\n",
        "This work led to the development of the important \"LeNet\" architecture used by Yann LeCun and collaborators at Bell Labs, where this \"local receptive field\" is extendended though multiple layers that \"stack up\" a increasingly \"zoomed out\" version of the image.  \n",
        "![lenet arch](https://cdn.inblog.in/user/uploads/b4b07bff68d8edf68622a0bb31b8aacd.png) Source: LeCun et al. \n",
        "\n",
        "At each stage, multiple \"filter banks\" exist, which filter different parts of the image.  As an illustration of this effect, consider this visualization of layer activations or \"feature maps\" at successive stages of a convolutional neural network being used for facial recognition:\n",
        "![layer feature maps](https://hedges.belmont.edu/scottergories/images/lee_et_all_faces.png) \n",
        "(Image source: Lee et al, 2009) \n",
        "\n",
        "Notice how the low-level features detect various kinds of edges (as with the cat neurons), which are combined via later layers in to \"mid-level features\" such as eyes and noses, and finally later layers for full face images.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51Q5accvql7Q"
      },
      "source": [
        "## So what's convolution?\n",
        "Convolution is a \"running\" dot product of a (typically smaller) vector or matrix called the \"kernel\" that is \"run over\" a larger vector or matrix representing a signal or image.  The important point is that it's the SAME set of weights being applied *all over* the image.  This is closely related to the notion that we often want our networks to be invariant to translations in space or time.  An example you're probably familiar with is  \"running average\", in which the kernel is a set of constant weights.  A different example would be a \"blur\" filter used in photo editing, in which a weighted local average of nearby points becomes the next point.\n",
        "\n",
        "The convolution kernel functions as a filter.  In the interactive demo below, convolution kernel in the middle is \"run over\" each point of the input image, and then the dot product of the (local) image pixels and the kernel becomes the new pixel in the new image. Try clicking on the point in the 3x3 kernel, and notice how the shape of the kernel tends to match whats \"allowed to pass through\" the filter to make up the output image:\n",
        "\n",
        "<iframe type=\"text/html\" src=\"https://hedges.belmont.edu/~shawley/acts/demo/demo_draw.html\" width=\"700px\" height=\"280px\" frameborder=\"0\">\n",
        " </iframe>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "8wZKPFe6r9Y_",
        "outputId": "b14c8385-4308-4d68-92db-180b32c2946c"
      },
      "source": [
        "from IPython.display import HTML, IFrame \n",
        "print(\"Choose a kernel preset from the drop-down or click on the squares to create your own kernel.\")\n",
        "HTML('<iframe type=\"text/html\" src=\"https://hedges.belmont.edu/~shawley/acts/demo/demo_draw.html\" width=\"700px\" height=\"280px\" frameborder=\"0\"></iframe>')\n",
        "#IFrame('https://hedges.belmont.edu/~shawley/acts/demo/demo_draw.html', width=700, height=280)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Choose a kernel preset from the drop-down or click on the squares to create your own kernel.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe type=\"text/html\" src=\"https://hedges.belmont.edu/~shawley/acts/demo/demo_draw.html\" width=\"700px\" height=\"280px\" frameborder=\"0\"></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCJHhbUetAJ3"
      },
      "source": [
        "With this conceptual understanding under your belt, take a look at [this great medium post by Daphne Cornelisse](https://www.freecodecamp.org/news/an-intuitive-guide-to-convolutional-neural-networks-260c2de0a050/) \n",
        "to fill in more details of how convolution works -- note that we're going to use PyTorch code instead of Keras. \n",
        "\n",
        "## Convolutions and Correlations\n",
        "As another way to think about convolutions -- and to give a 1D example (\"signal processing\") instead the 2D (\"computer vision\") exercises we've been doing, here's another interactive Javascript demo you can try, in the input signal is \"convolved\" with the second \"kernel\" signal.  The remainder of the demo shows the resulting filtered signal as well as the \"correlation cofficient\"  -- which is the familiar correlation coefficient \"R\" from other areas of statistics & science!  Notice that signals that \"make it through\" the kernel-filter are highly correlated, and vice versa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 670
        },
        "id": "onGdz4Qgsyd8",
        "outputId": "5dfb1eb8-73e7-451a-cb18-53f9853a6a80"
      },
      "source": [
        "IFrame('https://hedges.belmont.edu/signal_corr_trans.html', width=800, height=650)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"800\"\n",
              "            height=\"650\"\n",
              "            src=\"https://hedges.belmont.edu/signal_corr_trans.html\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7f410f032510>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyABQEw7pJ9H"
      },
      "source": [
        "# MNIST MLP & CNN Demo\n",
        "\n",
        "[MNIST](http://yann.lecun.com/exdb/mnist/) is a classic dataset of handwritten digits, which has been the testing ground for a [a variety of methods](https://en.wikipedia.org/wiki/MNIST_database)(Wikipedia) in machine learning.\n",
        "\n",
        "![MNIST example](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png)\n",
        "\n",
        "While Convolutional Neural Networks -- which we'll get to below -- tend to be most effective and for image processing, the 24x24-pixel images in MNIST are small enough that we can apply the 'single hidden layer' model  (also known as a Multi-Layer Perceptron or MLP)  to it.  \n",
        "\n",
        "Essentially, we will 'graduate' from the 7-segment display of digits from the previous lesson, to handwritten digits.  Then we'll move on to larger, more diverse image datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Q4wrna4uxae"
      },
      "source": [
        "## ~~First, check that we can use a GPU (runs faster)~~ From here down it's currently broken!  Workin' on it! Update soon!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DP8B5I2Lpqv6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "a8b5de1a-68e1-4da4-e875-115bd2197a2b"
      },
      "source": [
        "# First, confirm that Keras sees the GPU. Just execute this cell. \n",
        "from keras import backend as K\n",
        "assert len(K.tensorflow_backend._get_available_gpus()) > 0, \\\n",
        "  \"No GPU found. Go to Edit > Notebook Settings > Hardware Accelerator > GPU\"\n",
        "print(\"We're good: GPU found!\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-4d9dcdf3b49c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# First, confirm that Keras sees the GPU. Just execute this cell.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_available_gpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0;34m\"No GPU found. Go to Edit > Notebook Settings > Hardware Accelerator > GPU\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"We're good: GPU found!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'keras.backend' has no attribute 'tensorflow_backend'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcAkkUJiqcKr"
      },
      "source": [
        "## Download and prepare the data:\n",
        "\n",
        "We're going to write our neural networks using [Keras](http://keras.io), which also provides handy utility for downloading and setting up common ML datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKNyCH7hq-M5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "cea0f5d4-7893-494b-d6f7-5a912b471e30"
      },
      "source": [
        "import keras\n",
        "from keras.datasets import mnist\n",
        "\n",
        "# load the data, split between train and test sets\n",
        "(x_train, y_train), (x_val, y_val) = mnist.load_data()\n",
        "print(x_train.shape[0], 'train samples, ',x_val.shape[0], 'test samples')\n",
        "print('x_train.shape     = ',x_train.shape)\n",
        "img_rows, img_cols = x_train.shape[1], x_train.shape[2]  # save these for later\n",
        "\n",
        "# recast floating-point bits, and make pixel go from 0..1\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_val = x_val.astype('float32')  / 255\n",
        "\n",
        "# convert target class vectors to binary class matrices\n",
        "num_classes = 10              # ten classes: digts 0 to 9.\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_val = keras.utils.to_categorical(y_val, num_classes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n",
            "60000 train samples,  10000 test samples\n",
            "x_train.shape     =  (60000, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dh2AjzzurFB9"
      },
      "source": [
        "## Setup the MLP model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSjduXjQoHMr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "1e445522-3e3b-4807-9481-502ba34ca825"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.optimizers import RMSprop, Adam\n",
        "\n",
        "\n",
        "# For MLP model, we should reshape the input 24x24 images to 784x1 arrays \n",
        "# i.e., we will \"flatten\" each square image into a long line of pixels.\n",
        "# This will be our imput.\n",
        "x_train = x_train.reshape(x_train.shape[0], -1) # -1 just means make numpy figure out the rest of the shape\n",
        "x_val = x_val.reshape(x_val.shape[0], -1)\n",
        "print('new x_train.shape = ',x_train.shape)\n",
        "\n",
        "\n",
        "# Now define the Model \n",
        "mlp = Sequential()\n",
        "mlp.add(Dense(256, activation='relu', input_shape=(784,)))\n",
        "mlp.add(Dense(256, activation='relu'))\n",
        "mlp.add(Dense(num_classes, activation='softmax')) # softmax is like sigmoid but for when you want probabilities\n",
        "\n",
        "mlp.summary()  # print a summary\n",
        "\n",
        "mlp.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "new x_train.shape =  (60000, 784)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 256)               200960    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 269,322\n",
            "Trainable params: 269,322\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1qrqfciovZ2"
      },
      "source": [
        "## Train:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LEGihUpocvU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1159
        },
        "outputId": "a7c0a5d2-c2ea-4203-d56a-a2a08c19a443"
      },
      "source": [
        "batch_size = 128\n",
        "epochs = 20\n",
        "\n",
        "model = mlp \n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_val, y_val))\n",
        "score = model.evaluate(x_val, y_val, verbose=0)\n",
        "print('Val loss:', score[0])\n",
        "print('Val accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/30\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.2620 - acc: 0.9250 - val_loss: 0.1272 - val_acc: 0.9606\n",
            "Epoch 2/30\n",
            "60000/60000 [==============================] - 3s 47us/step - loss: 0.0958 - acc: 0.9705 - val_loss: 0.0897 - val_acc: 0.9715\n",
            "Epoch 3/30\n",
            "60000/60000 [==============================] - 3s 48us/step - loss: 0.0624 - acc: 0.9803 - val_loss: 0.0695 - val_acc: 0.9774\n",
            "Epoch 4/30\n",
            "60000/60000 [==============================] - 3s 47us/step - loss: 0.0426 - acc: 0.9867 - val_loss: 0.0768 - val_acc: 0.9755\n",
            "Epoch 5/30\n",
            "60000/60000 [==============================] - 3s 47us/step - loss: 0.0333 - acc: 0.9895 - val_loss: 0.0832 - val_acc: 0.9750\n",
            "Epoch 6/30\n",
            "60000/60000 [==============================] - 3s 46us/step - loss: 0.0243 - acc: 0.9924 - val_loss: 0.0650 - val_acc: 0.9795\n",
            "Epoch 7/30\n",
            "60000/60000 [==============================] - 3s 47us/step - loss: 0.0198 - acc: 0.9935 - val_loss: 0.0659 - val_acc: 0.9811\n",
            "Epoch 8/30\n",
            "60000/60000 [==============================] - 3s 47us/step - loss: 0.0156 - acc: 0.9946 - val_loss: 0.0756 - val_acc: 0.9792\n",
            "Epoch 9/30\n",
            "60000/60000 [==============================] - 3s 47us/step - loss: 0.0134 - acc: 0.9956 - val_loss: 0.0771 - val_acc: 0.9811\n",
            "Epoch 10/30\n",
            "60000/60000 [==============================] - 3s 45us/step - loss: 0.0114 - acc: 0.9963 - val_loss: 0.0843 - val_acc: 0.9776\n",
            "Epoch 11/30\n",
            "60000/60000 [==============================] - 3s 45us/step - loss: 0.0125 - acc: 0.9961 - val_loss: 0.0893 - val_acc: 0.9770\n",
            "Epoch 12/30\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 0.0129 - acc: 0.9955 - val_loss: 0.0907 - val_acc: 0.9785\n",
            "Epoch 13/30\n",
            "60000/60000 [==============================] - 3s 42us/step - loss: 0.0103 - acc: 0.9965 - val_loss: 0.0811 - val_acc: 0.9808\n",
            "Epoch 14/30\n",
            "60000/60000 [==============================] - 2s 42us/step - loss: 0.0080 - acc: 0.9974 - val_loss: 0.0837 - val_acc: 0.9825\n",
            "Epoch 15/30\n",
            "60000/60000 [==============================] - 2s 41us/step - loss: 0.0098 - acc: 0.9966 - val_loss: 0.0769 - val_acc: 0.9818\n",
            "Epoch 16/30\n",
            "60000/60000 [==============================] - 2s 41us/step - loss: 0.0098 - acc: 0.9966 - val_loss: 0.1009 - val_acc: 0.9790\n",
            "Epoch 17/30\n",
            "60000/60000 [==============================] - 2s 41us/step - loss: 0.0082 - acc: 0.9974 - val_loss: 0.0989 - val_acc: 0.9795\n",
            "Epoch 18/30\n",
            "60000/60000 [==============================] - 2s 41us/step - loss: 0.0087 - acc: 0.9972 - val_loss: 0.0933 - val_acc: 0.9808\n",
            "Epoch 19/30\n",
            "60000/60000 [==============================] - 2s 41us/step - loss: 0.0047 - acc: 0.9983 - val_loss: 0.0928 - val_acc: 0.9819\n",
            "Epoch 20/30\n",
            "60000/60000 [==============================] - 2s 41us/step - loss: 0.0073 - acc: 0.9977 - val_loss: 0.1020 - val_acc: 0.9788\n",
            "Epoch 21/30\n",
            "60000/60000 [==============================] - 2s 41us/step - loss: 0.0087 - acc: 0.9970 - val_loss: 0.1038 - val_acc: 0.9782\n",
            "Epoch 22/30\n",
            "60000/60000 [==============================] - 2s 40us/step - loss: 0.0066 - acc: 0.9979 - val_loss: 0.1031 - val_acc: 0.9793\n",
            "Epoch 23/30\n",
            "60000/60000 [==============================] - 2s 41us/step - loss: 0.0061 - acc: 0.9982 - val_loss: 0.1052 - val_acc: 0.9814\n",
            "Epoch 24/30\n",
            "60000/60000 [==============================] - 2s 41us/step - loss: 0.0071 - acc: 0.9977 - val_loss: 0.1000 - val_acc: 0.9789\n",
            "Epoch 25/30\n",
            "60000/60000 [==============================] - 3s 42us/step - loss: 0.0028 - acc: 0.9992 - val_loss: 0.0960 - val_acc: 0.9814\n",
            "Epoch 26/30\n",
            "60000/60000 [==============================] - 3s 42us/step - loss: 0.0046 - acc: 0.9987 - val_loss: 0.1129 - val_acc: 0.9808\n",
            "Epoch 27/30\n",
            "60000/60000 [==============================] - 3s 42us/step - loss: 0.0092 - acc: 0.9971 - val_loss: 0.1002 - val_acc: 0.9788\n",
            "Epoch 28/30\n",
            "60000/60000 [==============================] - 3s 42us/step - loss: 0.0029 - acc: 0.9991 - val_loss: 0.0985 - val_acc: 0.9821\n",
            "Epoch 29/30\n",
            "60000/60000 [==============================] - 2s 41us/step - loss: 0.0018 - acc: 0.9993 - val_loss: 0.1046 - val_acc: 0.9824\n",
            "Epoch 30/30\n",
            "60000/60000 [==============================] - 2s 41us/step - loss: 0.0074 - acc: 0.9977 - val_loss: 0.1016 - val_acc: 0.9819\n",
            "Val loss: 0.10162490810970662\n",
            "Val accuracy: 0.9819\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h40-nlgZw_mV"
      },
      "source": [
        "So we get an accuracy of around 98% with the MLP.    Let's try a Convolutional Neural Network (CNN or 'ConvNet') instead.\n",
        "\n",
        "# What is a CNN?\n",
        "\n",
        "Watch \n",
        "[this video by Luis Serrano](https://www.youtube.com/watch?v=2-Ol7ZB0MmU).\n",
        "\n",
        "The CNN is based on the visual cortex of the mamial brain, specifically cats.  Read this [writeup on ConvNets](https://ml4a.github.io/ml4a/convnets/).\n",
        "\n",
        "## Coding\n",
        "\n",
        "Writing your own CNN by hand in numpy can be a bit involved (if you want to try, you can [watch Siraj Raval explain how](https://www.youtube.com/watch?v=FTr3n7uBIuE), but not now), so we're going to rely on Keras. \n",
        "\n",
        "First we need to un-flatten the images that we flattened above for the MLP model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoLuM_nvsA-t"
      },
      "source": [
        "## Reshape the data 'back' for CNN model\n",
        "For the CNN model, we will use the images in their 24x24 form, but the Conv layers in Keras expect a 'color channel' as well, and it makes a difference what *order* the color channels are in, `channels_first` or `channels_last`... "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVTHJ_kiuBS7"
      },
      "source": [
        "if K.image_data_format() == 'channels_first':\n",
        "  input_shape = (1, img_rows, img_cols)   # '1' for greyscale images, 3 for RGB\n",
        "else:\n",
        "  input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.reshape(x_train.shape[0], input_shape[0], input_shape[1], input_shape[2])\n",
        "x_val = x_val.reshape(x_val.shape[0], input_shape[0], input_shape[1], input_shape[2])  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hv5MlknRu3Se"
      },
      "source": [
        "## Define the CNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dg_NCufApax1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "26e19993-7092-478e-f117-c5f481eb8587"
      },
      "source": [
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, BatchNormalization\n",
        "\n",
        "cnn = Sequential()\n",
        "cnn.add(Conv2D(32, kernel_size=(3, 3),activation='relu', input_shape=input_shape))\n",
        "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "cnn.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "cnn.add(Flatten())\n",
        "cnn.add(Dense(32, activation='relu'))\n",
        "cnn.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "cnn.summary()  # print a summary\n",
        "\n",
        "cnn.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_44 (Conv2D)           (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_26 (MaxPooling (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_45 (Conv2D)           (None, 11, 11, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_27 (MaxPooling (None, 5, 5, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten_19 (Flatten)         (None, 800)               0         \n",
            "_________________________________________________________________\n",
            "dense_79 (Dense)             (None, 32)                25632     \n",
            "_________________________________________________________________\n",
            "dense_80 (Dense)             (None, 10)                330       \n",
            "=================================================================\n",
            "Total params: 35,530\n",
            "Trainable params: 35,530\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QYTsFIbwUio"
      },
      "source": [
        "*^ Compare the number of parameters in this model to that of the MLP model *\n",
        "\n",
        "## And train as before..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwzyD9IDsbtR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "304a9eb6-64c7-4afb-a51f-36a0a4db8ac6"
      },
      "source": [
        "model = cnn \n",
        "epochs = 20\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_val, y_val))\n",
        "score = model.evaluate(x_val, y_val, verbose=0)\n",
        "print('Val loss:', score[0])\n",
        "print('Val accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/15\n",
            "60000/60000 [==============================] - 9s 152us/step - loss: 0.3113 - acc: 0.9020 - val_loss: 0.0835 - val_acc: 0.9718\n",
            "Epoch 2/15\n",
            "60000/60000 [==============================] - 6s 95us/step - loss: 0.0822 - acc: 0.9744 - val_loss: 0.0565 - val_acc: 0.9812\n",
            "Epoch 3/15\n",
            "60000/60000 [==============================] - 6s 95us/step - loss: 0.0580 - acc: 0.9818 - val_loss: 0.0409 - val_acc: 0.9864\n",
            "Epoch 4/15\n",
            "60000/60000 [==============================] - 6s 95us/step - loss: 0.0473 - acc: 0.9852 - val_loss: 0.0530 - val_acc: 0.9830\n",
            "Epoch 5/15\n",
            "60000/60000 [==============================] - 6s 96us/step - loss: 0.0391 - acc: 0.9878 - val_loss: 0.0350 - val_acc: 0.9884\n",
            "Epoch 6/15\n",
            "60000/60000 [==============================] - 6s 96us/step - loss: 0.0330 - acc: 0.9898 - val_loss: 0.0307 - val_acc: 0.9892\n",
            "Epoch 7/15\n",
            "60000/60000 [==============================] - 6s 95us/step - loss: 0.0278 - acc: 0.9918 - val_loss: 0.0318 - val_acc: 0.9893\n",
            "Epoch 8/15\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0253 - acc: 0.9920 - val_loss: 0.0401 - val_acc: 0.9870\n",
            "Epoch 9/15\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0224 - acc: 0.9928 - val_loss: 0.0367 - val_acc: 0.9878\n",
            "Epoch 10/15\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0200 - acc: 0.9938 - val_loss: 0.0284 - val_acc: 0.9917\n",
            "Epoch 11/15\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0181 - acc: 0.9945 - val_loss: 0.0363 - val_acc: 0.9888\n",
            "Epoch 12/15\n",
            "60000/60000 [==============================] - 6s 96us/step - loss: 0.0151 - acc: 0.9953 - val_loss: 0.0362 - val_acc: 0.9897\n",
            "Epoch 13/15\n",
            "60000/60000 [==============================] - 6s 96us/step - loss: 0.0128 - acc: 0.9962 - val_loss: 0.0352 - val_acc: 0.9891\n",
            "Epoch 14/15\n",
            "60000/60000 [==============================] - 6s 96us/step - loss: 0.0128 - acc: 0.9958 - val_loss: 0.0348 - val_acc: 0.9900\n",
            "Epoch 15/15\n",
            "60000/60000 [==============================] - 6s 95us/step - loss: 0.0117 - acc: 0.9962 - val_loss: 0.0328 - val_acc: 0.9898\n",
            "Val loss: 0.03280220323135509\n",
            "Val accuracy: 0.9898\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXxoym-401Ko"
      },
      "source": [
        "There are many variations we can apply to this model, adding layers with names such as \"Dropout\" and \"Batch Normalization\" too.  For now this will suffice.\n",
        "\n",
        "\n",
        "**Questions for in-class discussion:**\n",
        "1. If the CNN is supposed to be faster than the MLP for most applications, why is it *slower* on the MNIST dataset?\n",
        "2. ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVX3f_NXvNaL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}