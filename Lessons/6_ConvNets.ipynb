{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6_ConvNets.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x96fhjdvL0S"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1NxTAjQA3N9AQzS4VD_QlvDCOBm3by__u?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8STqvwWtosEm"
      },
      "source": [
        "# Convolutional Networks\n",
        "\n",
        "We've talked about the \"MLP\" architecture in previous lessons, in which inputs to the next layer of neurons are \"fully connected\" (everything is conntected to everything else). \n",
        "\n",
        "It turns out that for many applications in computer vision and signal processing, one can make great use of layers that look at only \"local\" (or \"nearby\") information, and then have the outpouts of those be combined at successively larger spatial (or temporal) scales.  This is what's known as a *local receptive field*, and is based directly on the biology of neurons in the visual cortex:\n",
        "\n",
        "![neuron receptive field](https://i.imgur.com/KC4PsPc.png)\n",
        "(Image source: http://neuroclusterbrain.com/neuron_model.html)\n",
        "\n",
        "This idea was exploided in the [\"Neocognitron\" model](https://link.springer.com/article/10.1007/BF00344251) which built off the Nobel-prize winning work of Hubel & Weisel with cat vision, and how the different neurons in the cat brain learn to function as \"pattern recognizers\" for things like edges:\n",
        "\n",
        "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/IOHayh06LJ4\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "5kN1ZYerpRFn",
        "outputId": "533148d2-0a7c-4358-97a8-70bbb1fc0bf2"
      },
      "source": [
        "from IPython.display import YouTubeVideo\n",
        "YouTubeVideo('IOHayh06LJ4')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"400\"\n",
              "            height=\"300\"\n",
              "            src=\"https://www.youtube.com/embed/IOHayh06LJ4\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.YouTubeVideo at 0x7f6585a16b90>"
            ],
            "image/jpeg": "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\n"
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nm3rp0qCpNdc"
      },
      "source": [
        "\n",
        "This work led to the development of the important \"LeNet\" architecture used by Yann LeCun and collaborators at Bell Labs, where this \"local receptive field\" is extendended though multiple layers that \"stack up\" a increasingly \"zoomed out\" version of the image.  \n",
        "![lenet arch](https://cdn.inblog.in/user/uploads/b4b07bff68d8edf68622a0bb31b8aacd.png) Source: LeCun et al. \n",
        "\n",
        "At each stage, multiple \"filter banks\" exist, which filter different parts of the image.  As an illustration of this effect, consider this visualization of layer activations or \"feature maps\" at successive stages of a convolutional neural network being used for facial recognition:\n",
        "![layer feature maps](https://hedges.belmont.edu/scottergories/images/lee_et_all_faces.png) \n",
        "(Image source: Lee et al, 2009) \n",
        "\n",
        "Notice how the low-level features detect various kinds of edges (as with the cat neurons), which are combined via later layers in to \"mid-level features\" such as eyes and noses, and finally later layers for full face images.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51Q5accvql7Q"
      },
      "source": [
        "## So what's convolution?\n",
        "Convolution is a \"running\" dot product of a (typically smaller) vector or matrix called the \"kernel\" that is \"run over\" a larger vector or matrix representing a signal or image.  The important point is that it's the SAME set of weights being applied *all over* the image.  This is closely related to the notion that we often want our networks to be invariant to translations in space or time.  An example you're probably familiar with is  \"running average\", in which the kernel is a set of constant weights.  A different example would be a \"blur\" filter used in photo editing, in which a weighted local average of nearby points becomes the next point.\n",
        "\n",
        "The convolution kernel functions as a filter.  In the interactive demo below, convolution kernel in the middle is \"run over\" each point of the input image, and then the dot product of the (local) image pixels and the kernel becomes the new pixel in the new image. Try clicking on the point in the 3x3 kernel, and notice how the shape of the kernel tends to match whats \"allowed to pass through\" the filter to make up the output image:\n",
        "\n",
        "<iframe type=\"text/html\" src=\"https://hedges.belmont.edu/~shawley/acts/demo/demo_draw.html\" width=\"700px\" height=\"280px\" frameborder=\"0\">\n",
        " </iframe>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "8wZKPFe6r9Y_",
        "outputId": "f7245453-91a5-4978-f1c8-7a0b45e333a0"
      },
      "source": [
        "from IPython.display import HTML, IFrame \n",
        "print(\"Choose a kernel preset from the drop-down or click on the squares to create your own kernel.\")\n",
        "HTML('<iframe type=\"text/html\" src=\"https://hedges.belmont.edu/~shawley/acts/demo/demo_draw.html\" width=\"700px\" height=\"280px\" frameborder=\"0\"></iframe>')\n",
        "#IFrame('https://hedges.belmont.edu/~shawley/acts/demo/demo_draw.html', width=700, height=280)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Choose a kernel preset from the drop-down or click on the squares to create your own kernel.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe type=\"text/html\" src=\"https://hedges.belmont.edu/~shawley/acts/demo/demo_draw.html\" width=\"700px\" height=\"280px\" frameborder=\"0\"></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCJHhbUetAJ3"
      },
      "source": [
        "With this conceptual understanding under your belt, take a look at [this great medium post by Daphne Cornelisse](https://www.freecodecamp.org/news/an-intuitive-guide-to-convolutional-neural-networks-260c2de0a050/) \n",
        "to fill in more details of how convolution works -- note that we're going to use PyTorch code instead of Keras. \n",
        "\n",
        "## Convolutions and Correlations\n",
        "As another way to think about convolutions -- and to give a 1D example (\"signal processing\") instead the 2D (\"computer vision\") exercises we've been doing, here's another interactive Javascript demo you can try, in the input signal is \"convolved\" with the second \"kernel\" signal.  The remainder of the demo shows the resulting filtered signal as well as the \"correlation cofficient\"  -- which is the familiar correlation coefficient \"R\" from other areas of statistics & science!  Notice that signals that \"make it through\" the kernel-filter are highly correlated, and vice versa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 670
        },
        "id": "onGdz4Qgsyd8",
        "outputId": "08a4fd38-2318-49fd-cb5a-9169c34837dd"
      },
      "source": [
        "IFrame('https://hedges.belmont.edu/signal_corr_trans.html', width=800, height=650)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"800\"\n",
              "            height=\"650\"\n",
              "            src=\"https://hedges.belmont.edu/signal_corr_trans.html\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7f6582fefc50>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyABQEw7pJ9H"
      },
      "source": [
        "# MNIST MLP & CNN Demo\n",
        "\n",
        "[MNIST](http://yann.lecun.com/exdb/mnist/) is a classic dataset of handwritten digits, which has been the testing ground for a [a variety of methods](https://en.wikipedia.org/wiki/MNIST_database)(Wikipedia) in machine learning.\n",
        "\n",
        "![MNIST example](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png)\n",
        "\n",
        "While Convolutional Neural Networks -- which we'll get to below -- tend to be most effective and for image processing, the 24x24-pixel images in MNIST are small enough that we can apply the 'single hidden layer' model  (also known as a Multi-Layer Perceptron or MLP)  to it.  \n",
        "\n",
        "Essentially, we will 'graduate' from the 7-segment display of digits from the previous lesson, to handwritten digits.  Then we'll move on to larger, more diverse image datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Q4wrna4uxae"
      },
      "source": [
        "## First, check that we can use a GPU (runs faster)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DP8B5I2Lpqv6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "feedda6f-0101-4a0a-d23b-65bd07a4c7bd"
      },
      "source": [
        "# First, confirm that we can see the GPU. Just execute this cell. \n",
        "import torch\n",
        "torch.cuda.is_available()   # If False, go up to Edit > Notebook Settings > Hardware Accelerator > GPU"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zW-mrUJUGUEv"
      },
      "source": [
        "# install some things\n",
        "! [ -e /content ] && pip install -Uqq fastai   # install upgrade fastai on colab\n",
        "! pip install mrspuff -Uq"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPqMSqYKF0xk"
      },
      "source": [
        "# let's import a bunch of other stuff\n",
        "import numpy as np \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch import optim\n",
        "from fastai.vision.all import *\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from fastai.learner import Learner\n",
        "from fastai.optimizer import OptimWrapper\n",
        "from fastai.callback.progress import *\n",
        "from functools import partial \n",
        "from IPython.display import display, HTML"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcAkkUJiqcKr"
      },
      "source": [
        "## Download and prepare the data:\n",
        "\n",
        "We're going to write our neural networks using [Keras](http://keras.io), which also provides handy utility for downloading and setting up common ML datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1m80aqS6HAhc",
        "outputId": "90c9fe75-8b9c-49e1-bf99-98f410c57b5a"
      },
      "source": [
        "# Download the data\n",
        "path = untar_data(URLs.MNIST)  # fastai stores the url in URLs.MNIST\n",
        "\n",
        "path.ls()  # let's see what's there. Note we could just as easily run \"! ls {path}\" instead"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#3) [Path('/root/.fastai/data/mnist_png/models'),Path('/root/.fastai/data/mnist_png/training'),Path('/root/.fastai/data/mnist_png/testing')]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6keSLpYHHFmL"
      },
      "source": [
        "So, there's a training set and a testing set.  Let's take a look in testing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsJLZi5aHJyZ",
        "outputId": "66f58510-afd2-46f8-d201-55cf8dd71050"
      },
      "source": [
        "(path/\"testing\").ls()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#10) [Path('/root/.fastai/data/mnist_png/testing/8'),Path('/root/.fastai/data/mnist_png/testing/9'),Path('/root/.fastai/data/mnist_png/testing/5'),Path('/root/.fastai/data/mnist_png/testing/4'),Path('/root/.fastai/data/mnist_png/testing/2'),Path('/root/.fastai/data/mnist_png/testing/6'),Path('/root/.fastai/data/mnist_png/testing/7'),Path('/root/.fastai/data/mnist_png/testing/1'),Path('/root/.fastai/data/mnist_png/testing/0'),Path('/root/.fastai/data/mnist_png/testing/3')]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v-3eaOQHPGX"
      },
      "source": [
        "...So there's a seperate directory for each digit (0 to 9).  Each digit will be regarded as a category for our classifier. And then in one of the digit directories:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3gYrvHsHxcN",
        "outputId": "67f038f2-abad-4d52-fbdb-4c1cdd6c75f0"
      },
      "source": [
        "(path/\"testing/8\").ls()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#974) [Path('/root/.fastai/data/mnist_png/testing/8/373.png'),Path('/root/.fastai/data/mnist_png/testing/8/3829.png'),Path('/root/.fastai/data/mnist_png/testing/8/3954.png'),Path('/root/.fastai/data/mnist_png/testing/8/8266.png'),Path('/root/.fastai/data/mnist_png/testing/8/9596.png'),Path('/root/.fastai/data/mnist_png/testing/8/8135.png'),Path('/root/.fastai/data/mnist_png/testing/8/8919.png'),Path('/root/.fastai/data/mnist_png/testing/8/7035.png'),Path('/root/.fastai/data/mnist_png/testing/8/1052.png'),Path('/root/.fastai/data/mnist_png/testing/8/2667.png')...]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upZtr7-lH5Cw"
      },
      "source": [
        "If we and to take a look at one, we go do..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 45
        },
        "id": "hU_9pv4XH7cz",
        "outputId": "f910beac-dc0f-4819-a44e-6d7b599f61fe"
      },
      "source": [
        "image = Image.open((path/\"testing/8\").ls()[0]) # this will take the first filename in the list and open it as an Image\n",
        "image   # have the notebook display the image -- it's an 8! (or it should be)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABHElEQVR4nM3QwSsEYRgG8McmO7WptQfKEEU5cHBydeM2xcFZ/gErTsKeXPwBixxcFJPitmVqLySrIWX3sOu4iT2wtFtiet7hss1kvu/o4D29Pb/e9+v9gP9c/ae+iKxqzbikkOS+SiOZKunVSdYGozYt5Et+ue+NlMmImUXh3iKw8sWDh0QEN8TfBmBcSzmlvGjTSQKweD8bhrGge3wHJg7RU1bRwcxmByyjcNFQD0mcCHcWPuRIJQBYa4rIk6lHjL+SxV9JW9ga1RR897zXzmkm0+RVgcwtaVYPN5iOx+e2KsyqOCreFACY2e91FfncWhHcE/7QPGLdnUgO7N7KnTJpkazYNZJnQ62oPUDX6fosjXnuTem4rrnlD+sHz+Z6JO3fpAAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28 at 0x7F64FCE8A1D0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeDowATlIJZ6",
        "outputId": "1e798eef-c483-489f-eaff-378b9ecde63d"
      },
      "source": [
        "image.size, image.mode   #  dimesions of image are 28x28.  'L' mode means grayscale"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((28, 28), 'L')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sFxwpbPJi-u"
      },
      "source": [
        "## Define the DataSets\n",
        "We're going to use fastai, which has various built-in methods for reading categorical data. Most of these methods assume our images are RGB rather than grayscale. In order to keep our image grayscale, we'll have to do a little extra work by defining a DataBlock, that will store our images, their categories, and how to load them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S99X54RpKH_w"
      },
      "source": [
        "mnist = DataBlock(blocks = (ImageBlock(cls=PILImageBW),CategoryBlock),\n",
        "                  get_items = get_image_files,\n",
        "                  splitter = GrandparentSplitter(train_name='training', valid_name='testing'),\n",
        "                  get_y = parent_label)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGGtidCqbPvx"
      },
      "source": [
        "batch_size = 64  # they're tiny, black & white images\n",
        "dls = mnist.dataloaders(path, bs=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEhquivnLEri"
      },
      "source": [
        "dls.show_batch()  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SACE7jaaSrfC"
      },
      "source": [
        "# let's check to make sure we're still grayscale:\n",
        "print(dls.train_ds[0][0].shape)   # should be (28,28)\n",
        "print(dls.one_batch()[0].shape)   # should have a 1 not a 3 for the second dimension"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XU1EJBH5JcFx"
      },
      "source": [
        "## Define a Model & Learner\n",
        "First we'll start with a *non*-convolutional model, an MLP:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pZuNQtSj1Dx"
      },
      "source": [
        "# i want to use torchinfo.summary so we can see how many parameters are in our model\n",
        "!pip install torchinfo \n",
        "from torchinfo import summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZX23ID7ZIYgW"
      },
      "source": [
        "# fastai does moves our model to the gpu automatically, but I want to use \n",
        "# torchinfo.summary first to see what our models' \"made of\", so we need to \n",
        "# know what device we're on.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# The PyTorch model looks similar:\n",
        "net = nn.Sequential(   # Sequential is a simplified interface that defines our .__init__() and .forward() method for us\n",
        "    nn.Flatten(),     # convert the data from being square to one big long line\n",
        "    nn.Linear(784, 256),  # 784 = 28x28  \n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, 128),   \n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128,10),     # ten categories for digits 0-9 on the output\n",
        "    nn.LogSoftmax(dim=1) # softmax is like sigmoid for multi-class. Log-softmax is more accurate as our \"Santa\" example indicated\n",
        "    ).to(device)\n",
        "\n",
        "summary(net, input_size=(batch_size, 1, 28, 28))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yO0WIcgkE77"
      },
      "source": [
        "Note (for later) that there's 235,000 trainable parameters in this model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfeKkT45L3xa"
      },
      "source": [
        "Oh, remember WandB!  Yea, we should use it!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfs8Ubl0Lq36"
      },
      "source": [
        "use_wandb = False\n",
        "if use_wandb:\n",
        "    !pip install wandb -qqq\n",
        "    import wandb\n",
        "    from fastai.callback.wandb import *\n",
        "    wandb.login()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xjp9NJnKvTF"
      },
      "source": [
        "if use_wandb:\n",
        "    wandb.finish()  # stop any aborted previous runs (I made a lot of mistake preparing this!)\n",
        "    wandb.init(project='mnist_lesson6')\n",
        "    cbs = WandbCallback()\n",
        "else:\n",
        "    cbs = []\n",
        "\n",
        "opt_func = partial(OptimWrapper, opt=optim.Adam)  # Optimizer method\n",
        "\n",
        "\n",
        "# NLLLoss is negative log-likelihood, and it pairs with our LogSoftMax activation\n",
        "learn = Learner(dls, net, metrics=accuracy, opt_func=opt_func, loss_func=nn.NLLLoss(), cbs=cbs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nIYu5u6M5pD"
      },
      "source": [
        "#what learning rate to use?  Let's let the LR Finder tell us:\n",
        "learn.lr_find()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0djTMvGBeEX5"
      },
      "source": [
        "learn.fit(10, lr=1e-3)\n",
        "if use_wandb: wandb.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h40-nlgZw_mV"
      },
      "source": [
        "So we get an accuracy of around 97% with our MLP, along with a validation loss of 0.13, and a much lower training loss -- indicating that our model is significantly *overfitting*.  \n",
        "\n",
        " Let's try a Convolutional Neural Network (CNN or 'ConvNet') instead.\n",
        "\n",
        "The standard fastai convnet architectures are known as ResNets, which come in different sizes (e.g. resnet18, resnet34).  \n",
        "\n",
        "But we're going to define our own convnet..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hv5MlknRu3Se"
      },
      "source": [
        "## Define the CNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXajcaI5ivSG"
      },
      "source": [
        "class ConvNet(nn.Module):\n",
        "    # from PyTorch tutorial, https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/convolutional_neural_network/main.py#L36\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ConvNet, self).__init__()\n",
        "        # We have the option to build our network in \"blocks\". \n",
        "        #   We don't have to, but let's try it\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
        "            #nn.BatchNorm2d(16),  # Try coming back and uncommenting these BatchNorm lines and re-training! ;-) \n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
        "            #nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.fc = nn.Linear(7*7*32, num_classes)  #  a final fully connected layer\n",
        "        self.act = nn.LogSoftmax(dim=1) # as I was writing this I forgot the softmax and nothing converged. ;-) \n",
        "\n",
        "    def forward(self, x, pretrained=False):\n",
        "        out = self.block1(x)\n",
        "        out = self.block2(out)\n",
        "        out = out.reshape(out.size(0), -1) # this is like nn.Flatten()\n",
        "        out = self.fc(out)\n",
        "        out = self.act(out)\n",
        "        return out\n",
        "\n",
        "cnn = ConvNet().to(device)\n",
        "summary(cnn, input_size=(batch_size, 1, 28, 28))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4z-sJe5QkTDU"
      },
      "source": [
        "...So, only 29,000 parameters instead of the MLP's 230,000!. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QYTsFIbwUio"
      },
      "source": [
        "\n",
        "## And train as before..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lM1ZSLLZiAGf"
      },
      "source": [
        "if use_wandb:\n",
        "    wandb.finish() \n",
        "    wandb.init(project='mnist_lesson6')\n",
        "\n",
        "learn = Learner(dls, cnn, metrics=accuracy, opt_func=opt_func, loss_func=nn.NLLLoss(), cbs=cbs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zd2wYjpZlDOI"
      },
      "source": [
        "learn.lr_find()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyrDeg9xh7br"
      },
      "source": [
        "learn.fit(10, lr=1e-3)\n",
        "if use_wandb: wandb.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYFtYA_opu0S"
      },
      "source": [
        "The loss values with the CNN were quite a bit lower than they were with the MLP, yes?  Correspondingly, we scored higher in accuracy too. And while training loss validation losses remained more commensurate -- less overfitting!  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXxoym-401Ko"
      },
      "source": [
        "There are many variations we can apply to this model, adding layers with names such as \"Dropout\" and \"Batch Normalization\" too.  For now this will suffice.\n",
        "\n",
        "\n",
        "**Questions for in-class discussion:**\n",
        "1. If the CNN is supposed to be faster than the MLP for *most* applications, why is it *no faster* on the MNIST dataset?\n",
        "2. Did you try the BatchNorm?  What did it do?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVX3f_NXvNaL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}